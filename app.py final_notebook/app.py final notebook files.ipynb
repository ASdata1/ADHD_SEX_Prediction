{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e57c5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(r\"C:\\Users\\04ama\\Downloads\\raw adhd data\\raw_data.csv\")\n",
    "\n",
    "# Feature engineering\n",
    "def handedness_group(score):\n",
    "    # Remove the pd.isnull check - let KNN handle missing values\n",
    "    if score <= -0.5:\n",
    "        return 0.0  # Left\n",
    "    elif score >= 0.5:\n",
    "        return 2.0  # Right  \n",
    "    else:\n",
    "        return 1.0  # Ambidextrous\n",
    "\n",
    "# Create handedness group as numeric from the start\n",
    "df['Handedness_Group'] = df['EHQ_EHQ_Total'].apply(handedness_group)\n",
    "\n",
    "# Remove MRI_Track_Age_at_Scan\n",
    "if 'MRI_Track_Age_at_Scan' in df.columns:\n",
    "    df = df.drop(columns=['MRI_Track_Age_at_Scan'])\n",
    "\n",
    "# Identify quantitative and categorical columns (excluding target and participant_id)\n",
    "quant_cols = [col for col in df.columns if col.startswith('APQ_') or col.startswith('SDQ_') or col.startswith('EHQ_') or col.startswith('ColorVision')]\n",
    "cat_cols = [col for col in df.columns if col.startswith('PreInt_') or col.startswith('Basic_') or 'Handedness' in col]\n",
    "conn_cols = list(df.iloc[:, 1:19902].columns)\n",
    "\n",
    "# Remove columns with high correlation (>=0.7)\n",
    "quant_df = df[quant_cols].copy()\n",
    "corr = quant_df.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.7)]\n",
    "df = df.drop(columns=to_drop)\n",
    "quant_cols = [col for col in quant_cols if col not in to_drop]\n",
    "\n",
    "# Save column lists\n",
    "with open('quant_cols.json', 'w') as f:\n",
    "    json.dump(quant_cols, f)\n",
    "with open('cat_cols.json', 'w') as f:\n",
    "    json.dump(cat_cols, f)\n",
    "with open('conn_cols.json', 'w') as f:\n",
    "    json.dump(conn_cols, f)\n",
    "\n",
    "# 1. Impute missing values\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[quant_cols + cat_cols] = imputer.fit_transform(df[quant_cols + cat_cols])\n",
    "joblib.dump(imputer, 'imputer.joblib')\n",
    "\n",
    "# After imputation\n",
    "print(\"\\n2. AFTER IMPUTATION:\")\n",
    "print(f\"Quant cols NaN: {df[quant_cols].isnull().sum().sum()}\")\n",
    "print(f\"Cat cols NaN: {df[cat_cols].isnull().sum().sum()}\")\n",
    "\n",
    "# 2. Scale quantitative columns\n",
    "scaler = StandardScaler()\n",
    "df[quant_cols] = scaler.fit_transform(df[quant_cols])\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "\n",
    "# 3. One-hot encode categorical columns\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(df[cat_cols])\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out(cat_cols), index=df.index)\n",
    "df = df.drop(columns=cat_cols)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "joblib.dump(encoder, 'encoder.joblib')\n",
    "\n",
    "# 4. PCA for connectome\n",
    "if len(conn_cols) > 0:\n",
    "    pca = PCA(n_components=10)\n",
    "    conn_pca = pca.fit_transform(df[conn_cols])\n",
    "    pca_cols = [f'conn_pca_{i+1}' for i in range(10)]\n",
    "    conn_pca_df = pd.DataFrame(conn_pca, columns=pca_cols, index=df.index)\n",
    "    df = df.drop(columns=conn_cols)\n",
    "    df = pd.concat([df, conn_pca_df], axis=1)\n",
    "    joblib.dump(pca, 'pca_connectome.joblib')\n",
    "else:\n",
    "    pca_cols = []\n",
    "\n",
    "# Final NaN check\n",
    "print(f\"Final NaN check: {df.isnull().sum().sum()}\")\n",
    "if df.isnull().sum().sum() > 0:\n",
    "    print(\"Filling remaining NaN values...\")\n",
    "    # Fill numeric columns with median, others with 0\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "    df = df.fillna(0)  # Fill any remaining non-numeric NaN\n",
    "    print(f\"NaN after final cleanup: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# 4. Modelling\n",
    "target_col = 'ADHD_Outcome'\n",
    "X = df.drop(columns=[target_col, 'participant_id'], errors='ignore')\n",
    "y = df[target_col]\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Logistic Regression with GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set with custom threshold\n",
    "y_test_proba = grid_search.best_estimator_.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.45\n",
    "y_test_pred_custom = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "report = classification_report(y_test, y_test_pred_custom)\n",
    "print(report)\n",
    "cm = confusion_matrix(y_test, y_test_pred_custom)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(\"Macro F1-score:\", f1_score(y_test, y_test_pred_custom, average='macro'))\n",
    "\n",
    "# Save model and threshold\n",
    "joblib.dump(grid_search.best_estimator_, 'logistic_regression_model.joblib')\n",
    "with open('adhd_lr_threshold.json', 'w') as f:\n",
    "    json.dump({'threshold': threshold}, f)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
