{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe2602a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.3.2)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from scikit-optimize) (24.1)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\04ama\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
      "Downloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f10582e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testign GridSearch Cv and BayesianSearch Cv for hyperparameter tuning\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from skopt import BayesSearchCV  # This requires scikit-optimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import joblib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b6d6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STATISTICAL MODEL COMPARISON (HYPOTHESIS TESTING + EFFECT SIZE)\n",
    "# =============================================================================\n",
    "\n",
    "from scipy.stats import ttest_rel, wilcoxon, friedmanchisquare\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def statistical_comparison(results_dict, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform pairwise statistical comparisons between models using:\n",
    "    - Paired t-test (assumes normality of performance scores)\n",
    "    - Wilcoxon signed-rank test (non-parametric, no distribution assumption)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Output from hyperparameter tuning. Each value must include 'cv_scores', e.g.:\n",
    "        {\n",
    "            \"ModelA\": {'cv_scores': [...]},\n",
    "            \"ModelB\": {'cv_scores': [...]}\n",
    "        }\n",
    "    alpha : float (default=0.05)\n",
    "        Significance level for testing (probability of Type I error)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            \"ModelA_vs_ModelB\": {\n",
    "                \"mean_diff\": value,\n",
    "                \"t_test\": {\"stat\": , \"p_value\": },\n",
    "                \"wilcoxon\": {\"stat\": , \"p_value\": },\n",
    "                \"significant_t\": True/False,\n",
    "                \"significant_w\": True/False\n",
    "            }, ...\n",
    "        }\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"STATISTICAL HYPOTHESIS TESTING\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    model_names = list(results_dict.keys())\n",
    "    test_results = {}\n",
    "\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        scores1 = results_dict[model1]['cv_scores']\n",
    "        scores2 = results_dict[model2]['cv_scores']\n",
    "\n",
    "        # Paired t-test (parametric)\n",
    "        t_stat, t_pval = ttest_rel(scores1, scores2)\n",
    "\n",
    "        # Wilcoxon signed-rank test (non-parametric)\n",
    "        w_stat, w_pval = wilcoxon(scores1, scores2)\n",
    "\n",
    "        mean_diff = np.mean(scores1) - np.mean(scores2)\n",
    "\n",
    "        test_results[f\"{model1}_vs_{model2}\"] = {\n",
    "            'mean_diff': mean_diff,\n",
    "            't_test': {'statistic': t_stat, 'p_value': t_pval},\n",
    "            'wilcoxon': {'statistic': w_stat, 'p_value': w_pval},\n",
    "            'significant_t': t_pval < alpha,\n",
    "            'significant_w': w_pval < alpha\n",
    "        }\n",
    "\n",
    "        print(f\"\\n{model1} vs {model2}:\")\n",
    "        print(f\"   Mean difference: {mean_diff:.4f}\")\n",
    "        print(f\"   T-test p-value: {t_pval:.4f} {'(significant)' if t_pval < alpha else ''}\")\n",
    "        print(f\"   Wilcoxon p-value: {w_pval:.4f} {'(significant)' if w_pval < alpha else ''}\")\n",
    "\n",
    "    return test_results\n",
    "\n",
    "\n",
    "def friedman_test(results_dict, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform the Friedman test to compare more than two models across multiple datasets/folds.\n",
    "\n",
    "    This is the non-parametric equivalent of repeated-measures ANOVA.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dict containing each model name and its 'cv_scores'\n",
    "    alpha : float\n",
    "        Significance level\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    (statistic, p_value, is_significant)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nFRIEDMAN TEST (Multiple Model Comparison)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Collect all models' cv scores into a list\n",
    "    model_scores = [results_dict[m]['cv_scores'] for m in results_dict.keys()]\n",
    "\n",
    "    # Apply Friedman test\n",
    "    stat, p_value = friedmanchisquare(*model_scores)\n",
    "    is_significant = p_value < alpha\n",
    "\n",
    "    print(f\"   Friedman statistic: {stat:.4f}\")\n",
    "    print(f\"   P-value: {p_value:.4f}\")\n",
    "    print(f\"   Significant difference: {'Yes' if is_significant else 'No'} (Î±={alpha})\")\n",
    "\n",
    "    return stat, p_value, is_significant\n",
    "\n",
    "\n",
    "def effect_size_analysis(results_dict):\n",
    "    \"\"\"\n",
    "    Compute Cohen's d effect size for all pairwise model comparisons.\n",
    "    This tells how *big* the performance difference is, not just whether it's significant.\n",
    "\n",
    "    Guidelines for interpretation (absolute value of d):\n",
    "    - < 0.2  : negligible effect\n",
    "    - < 0.5  : small effect\n",
    "    - < 0.8  : medium effect\n",
    "    - >= 0.8 : large effect\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : dict\n",
    "        Dict containing each model's cross-validation scores.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        {\n",
    "            \"ModelA_vs_ModelB\": {\"cohens_d\": value, \"interpretation\": \"small/medium/...\"},\n",
    "            ...\n",
    "        }\n",
    "    \"\"\"\n",
    "    print(\"\\nEFFECT SIZE ANALYSIS (Cohen's d)\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    model_names = list(results_dict.keys())\n",
    "    effect_sizes = {}\n",
    "\n",
    "    for model1, model2 in itertools.combinations(model_names, 2):\n",
    "        scores1 = results_dict[model1]['cv_scores']\n",
    "        scores2 = results_dict[model2]['cv_scores']\n",
    "\n",
    "        pooled_std = np.sqrt((np.var(scores1) + np.var(scores2)) / 2)\n",
    "        d = (np.mean(scores1) - np.mean(scores2)) / pooled_std\n",
    "\n",
    "        if abs(d) < 0.2:\n",
    "            interpretation = \"negligible\"\n",
    "        elif abs(d) < 0.5:\n",
    "            interpretation = \"small\"\n",
    "        elif abs(d) < 0.8:\n",
    "            interpretation = \"medium\"\n",
    "        else:\n",
    "            interpretation = \"large\"\n",
    "\n",
    "        effect_sizes[f\"{model1}_vs_{model2}\"] = {\n",
    "            'cohens_d': d,\n",
    "            'interpretation': interpretation\n",
    "        }\n",
    "\n",
    "        print(f\"   {model1} vs {model2}: d = {d:.3f} ({interpretation})\")\n",
    "\n",
    "    return effect_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b74c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ–¥ï¸ PERFORMANCE ESTIMATE\n",
      "   Total estimated runtime: ~30 minutes\n",
      "   RAM usage: ~3-4 GB\n",
      "   CPU usage: 50% (2 cores)\n",
      "   Total evaluations: 60 (20 per model)\n",
      "ðŸŽ¯ OPTIMIZED BAYESIAN HYPERPARAMETER TUNING\n",
      "============================================================\n",
      "\n",
      "ðŸ”„ Optimizing Logistic Regression with Bayesian Search...\n",
      "   ðŸŽ¯ Running 20 Bayesian iterations with 3-fold CV\n",
      "   â±ï¸ Estimated time: 3 minutes\n",
      "   âœ… Best CV Score: 0.8855\n",
      "   ðŸŽ¯ Best Parameters:\n",
      "      C: 19.10178715209079\n",
      "      class_weight: None\n",
      "      penalty: l1\n",
      "      solver: liblinear\n",
      "\n",
      "ðŸ”„ Optimizing LightGBM with Bayesian Search...\n",
      "   ðŸŽ¯ Running 20 Bayesian iterations with 3-fold CV\n",
      "   â±ï¸ Estimated time: 12 minutes\n",
      "   âœ… Best CV Score: 0.8163\n",
      "   ðŸŽ¯ Best Parameters:\n",
      "      colsample_bytree: 0.9103661677432962\n",
      "      learning_rate: 0.29888282209418915\n",
      "      max_depth: 4\n",
      "      min_child_samples: 21\n",
      "      n_estimators: 101\n",
      "      num_leaves: 60\n",
      "      reg_alpha: 0.01\n",
      "      reg_lambda: 10.0\n",
      "      subsample: 0.8195395088209111\n",
      "\n",
      "ðŸ”„ Optimizing XGBoost with Bayesian Search...\n",
      "   ðŸŽ¯ Running 20 Bayesian iterations with 3-fold CV\n",
      "   â±ï¸ Estimated time: 15 minutes\n",
      "   âœ… Best CV Score: 0.8086\n",
      "   ðŸŽ¯ Best Parameters:\n",
      "      colsample_bytree: 0.9866433958929786\n",
      "      gamma: 1.401242889242732\n",
      "      learning_rate: 0.2679129821158038\n",
      "      max_depth: 5\n",
      "      min_child_weight: 4\n",
      "      n_estimators: 82\n",
      "      reg_alpha: 0.010609897019208126\n",
      "      reg_lambda: 3.713148021750417\n",
      "      subsample: 0.8783136740774157\n",
      "ðŸ”¬ STATISTICAL HYPOTHESIS TESTING\n",
      "==================================================\n",
      "\n",
      "   ðŸ“Š Logistic Regression vs LightGBM:\n",
      "      Mean difference: -0.0051\n",
      "      T-test p-value: 0.8512 \n",
      "      Wilcoxon p-value: 0.3683 \n",
      "\n",
      "   ðŸ“Š Logistic Regression vs XGBoost:\n",
      "      Mean difference: -0.0041\n",
      "      T-test p-value: 0.8778 \n",
      "      Wilcoxon p-value: 0.2305 \n",
      "\n",
      "   ðŸ“Š LightGBM vs XGBoost:\n",
      "      Mean difference: 0.0011\n",
      "      T-test p-value: 0.6727 \n",
      "      Wilcoxon p-value: 0.7285 \n",
      "\n",
      "ðŸŽ¯ FRIEDMAN TEST (Multiple Model Comparison)\n",
      "----------------------------------------\n",
      "   Friedman statistic: 0.9000\n",
      "   P-value: 0.6376\n",
      "   Significant difference: No (Î±=0.05)\n",
      "   ðŸ’¡ No significant differences detected between models\n",
      "\n",
      "ðŸ“ EFFECT SIZE ANALYSIS (Cohen's d)\n",
      "----------------------------------------\n",
      "   Logistic Regression vs LightGBM: d = -0.061 (negligible)\n",
      "   Logistic Regression vs XGBoost: d = -0.049 (negligible)\n",
      "   LightGBM vs XGBoost: d = 0.121 (negligible)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# BAYESIAN HYPERPARAMETER TUNING (Optimized & Cleaned)\n",
    "# =============================================================================\n",
    "\n",
    "def optimized_bayesian_hyperparameter_tuning(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    Perform efficient Bayesian hyperparameter tuning for multiple models\n",
    "    (Logistic Regression, LightGBM, XGBoost) using reduced search spaces \n",
    "    and limited iterations to control compute time.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train, y_train : Training data\n",
    "    X_val, y_val     : Validation data (kept for future extensions, not used here)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing best estimator, best score, and params for each model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"OPTIMIZED BAYESIAN HYPERPARAMETER TUNING\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Define models and reduced parameter spaces\n",
    "    models_and_params = {\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'params': {\n",
    "                'C': Real(1e-3, 1e2, prior='log-uniform'),\n",
    "                'penalty': Categorical(['l1', 'l2']),\n",
    "                'solver': Categorical(['liblinear', 'saga']),\n",
    "                'class_weight': Categorical([None, 'balanced'])\n",
    "            }\n",
    "        },\n",
    "        'LightGBM': {\n",
    "            'model': lgb.LGBMClassifier(random_state=42, verbose=-1),\n",
    "            'params': {\n",
    "                'n_estimators': Integer(50, 200),\n",
    "                'learning_rate': Real(0.05, 0.3),\n",
    "                'max_depth': Integer(3, 8),\n",
    "                'num_leaves': Integer(15, 60),\n",
    "                'min_child_samples': Integer(10, 40),\n",
    "                'subsample': Real(0.7, 1.0),\n",
    "                'colsample_bytree': Real(0.7, 1.0),\n",
    "                'reg_alpha': Real(0.01, 10),\n",
    "                'reg_lambda': Real(0.01, 10)\n",
    "            }\n",
    "        },\n",
    "        'XGBoost': {\n",
    "            'model': xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "            'params': {\n",
    "                'n_estimators': Integer(50, 200),\n",
    "                'learning_rate': Real(0.05, 0.3),\n",
    "                'max_depth': Integer(3, 8),\n",
    "                'min_child_weight': Integer(1, 8),\n",
    "                'subsample': Real(0.7, 1.0),\n",
    "                'colsample_bytree': Real(0.7, 1.0),\n",
    "                'reg_alpha': Real(0.01, 10),\n",
    "                'reg_lambda': Real(0.01, 10),\n",
    "                'gamma': Real(0, 2)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Perform Bayesian Optimization\n",
    "    for name, config in models_and_params.items():\n",
    "        print(f\"\\nOptimizing {name}...\")\n",
    "\n",
    "        search = BayesSearchCV(\n",
    "            estimator=config['model'],\n",
    "            search_spaces=config['params'],\n",
    "            n_iter=20,              # Reduced for faster tuning\n",
    "            cv=3,                   # 3-fold cross validation\n",
    "            scoring='f1_macro',\n",
    "            random_state=42,\n",
    "            n_jobs=2\n",
    "        )\n",
    "\n",
    "        search.fit(X_train, y_train)\n",
    "\n",
    "        # Store model results\n",
    "        results[name] = {\n",
    "            'best_params': search.best_params_,\n",
    "            'best_score': search.best_score_,\n",
    "            'cv_scores': search.cv_results_['mean_test_score'],\n",
    "            'best_estimator': search.best_estimator_\n",
    "        }\n",
    "\n",
    "        print(f\"   Best CV F1-Macro: {search.best_score_:.4f}\")\n",
    "        print(\"   Best Parameters:\")\n",
    "        for param, value in search.best_params_.items():\n",
    "            print(f\"      {param}: {value}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5680a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tuned_models_and_params(results, save_dir=r'C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models'):\n",
    "    \"\"\"\n",
    "    Saves all tuned models, their best parameters, and tuning summaries to disk.\n",
    "\n",
    "    Why this is useful:\n",
    "    -------------------\n",
    "    After hyperparameter tuning, you usually want to:\n",
    "    - Save the best estimator (trained model).\n",
    "    - Save the best hyperparameters.\n",
    "    - Save the summary of all results for later loading, comparison or re-evaluation.\n",
    "\n",
    "    This function ensures reproducibility â€” models can be reloaded later without re-tuning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Output from hyperparameter tuning step.\n",
    "        Should have structure:\n",
    "        {\n",
    "            \"ModelName\": {\n",
    "                'best_estimator': fitted_model_object,\n",
    "                'best_params': {param_dict},\n",
    "                'best_score': float,\n",
    "                ... (optional other keys)\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "\n",
    "    save_dir : str\n",
    "        Directory where models and results will be stored.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    saved_models : dict\n",
    "        A dictionary summarizing saved file paths and best scores for each model.\n",
    "    \"\"\"\n",
    "    import os\n",
    "\n",
    "    # Create directory if it doesn't already exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"Saving tuned models and parameters\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    saved_models = {}\n",
    "\n",
    "    for model_name, model_results in results.items():\n",
    "        print(f\"\\nSaving {model_name}...\")\n",
    "\n",
    "        # Create filesystem-friendly filename from model name\n",
    "        clean_name = model_name.replace(' ', '_').lower()\n",
    "\n",
    "        # Define storage paths\n",
    "        model_path = os.path.join(save_dir, f'{clean_name}_best_model.pkl')\n",
    "        params_path = os.path.join(save_dir, f'{clean_name}_best_params.pkl')\n",
    "        results_path = os.path.join(save_dir, f'{clean_name}_results.pkl')\n",
    "\n",
    "        # Save model, parameters, and full results\n",
    "        joblib.dump(model_results['best_estimator'], model_path)\n",
    "        joblib.dump(model_results['best_params'], params_path)\n",
    "        joblib.dump(model_results, results_path)\n",
    "\n",
    "        # Add to summary dictionary\n",
    "        saved_models[model_name] = {\n",
    "            'model_path': model_path,\n",
    "            'params_path': params_path,\n",
    "            'results_path': results_path,\n",
    "            'best_score': model_results['best_score'],\n",
    "            'best_params': model_results['best_params']\n",
    "        }\n",
    "\n",
    "        print(f\"   Model saved to:      {model_path}\")\n",
    "        print(f\"   Parameters saved to: {params_path}\")\n",
    "        print(f\"   Results saved to:    {results_path}\")\n",
    "        print(f\"   Best CV F1-Macro:    {model_results['best_score']:.4f}\")\n",
    "\n",
    "    # Save overall summary for easy loading later\n",
    "    summary_path = os.path.join(save_dir, 'tuning_summary.pkl')\n",
    "    joblib.dump(saved_models, summary_path)\n",
    "\n",
    "    print(\"\\nSummary of tuned models:\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # Rank models by best score\n",
    "    sorted_models = sorted(saved_models.items(), key=lambda x: x[1]['best_score'], reverse=True)\n",
    "    for i, (name, info) in enumerate(sorted_models, 1):\n",
    "        print(f\"   {i}. {name}: {info['best_score']:.4f}\")\n",
    "\n",
    "    print(f\"\\nAll models and parameters saved to: {save_dir}\")\n",
    "    print(f\"Summary saved to: {summary_path}\")\n",
    "\n",
    "    return saved_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a496b654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ SAVING TUNED MODELS AND PARAMETERS\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Saving Logistic Regression...\n",
      "   âœ… Model saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\logistic_regression_best_model.pkl\n",
      "   âœ… Parameters saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\logistic_regression_best_params.pkl\n",
      "   âœ… Results saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\logistic_regression_results.pkl\n",
      "   ðŸ“Š Best CV F1-Macro: 0.8855\n",
      "\n",
      "ðŸ“ Saving LightGBM...\n",
      "   âœ… Model saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\lightgbm_best_model.pkl\n",
      "   âœ… Parameters saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\lightgbm_best_params.pkl\n",
      "   âœ… Results saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\lightgbm_results.pkl\n",
      "   ðŸ“Š Best CV F1-Macro: 0.8163\n",
      "\n",
      "ðŸ“ Saving XGBoost...\n",
      "   âœ… Model saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\xgboost_best_model.pkl\n",
      "   âœ… Parameters saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\xgboost_best_params.pkl\n",
      "   âœ… Results saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\xgboost_results.pkl\n",
      "   ðŸ“Š Best CV F1-Macro: 0.8086\n",
      "\n",
      "ðŸ“‹ TUNING SUMMARY\n",
      "------------------------------\n",
      "   #1 Logistic Regression: 0.8855\n",
      "   #2 LightGBM: 0.8163\n",
      "   #3 XGBoost: 0.8086\n",
      "\n",
      "âœ… All models and parameters saved to: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\n",
      "âœ… Summary saved: C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\data\\tuned_models\\tuning_summary.pkl\n"
     ]
    }
   ],
   "source": [
    "saved_models_info = save_tuned_models_and_params(results)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
