{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a99c885f",
   "metadata": {},
   "source": [
    "## Uploading Data \n",
    "\n",
    "Uploading Categorical, Quantitative and Connectome Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e821b4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADHD Prediction Dataset - Data Merging Module\n",
    "\n",
    "This notebook merges multiple ADHD-related datasets into a single comprehensive dataset\n",
    "for machine learning analysis. The merged dataset combines neuroimaging connectome data,\n",
    "behavioral questionnaires, demographic information, and target labels.\n",
    "\n",
    "\n",
    "Project: ADHD Sex Prediction\n",
    "Input Files: 4 separate datasets (connectome, quantitative, categorical, targets)\n",
    "Output: Unified raw dataset for preprocessing pipeline\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND FILE PATHS\n",
    "# =============================================================================\n",
    "\n",
    "# Data file paths\n",
    "DATA_DIR = Path(\"C:/Users/04ama/Downloads\")\n",
    "OUTPUT_DIR = Path(\".\")\n",
    "\n",
    "# Input file paths\n",
    "CONNECTOME_FILE = DATA_DIR / \"TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson (1).csv\"\n",
    "QUANTITATIVE_FILE = DATA_DIR / \"TRAIN_QUANTITATIVE_METADATA_new.xlsx\"\n",
    "CATEGORICAL_FILE = DATA_DIR / \"TRAIN_CATEGORICAL_METADATA_new.xlsx\"\n",
    "TARGET_FILE = DATA_DIR / \"TRAINING_SOLUTIONS (1).xlsx\"\n",
    "\n",
    "# Output file\n",
    "OUTPUT_FILE = OUTPUT_DIR / \"raw_dataset.csv\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25e6f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DATA LOADING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def load_dataset(file_path, dataset_name, file_type='csv'):\n",
    "    \"\"\"\n",
    "    Load a dataset with error handling and basic validation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    file_path : str or Path\n",
    "        Path to the data file\n",
    "    dataset_name : str\n",
    "        Human-readable name for logging\n",
    "    file_type : str\n",
    "        File format ('csv' or 'excel')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Loaded dataset\n",
    "    \n",
    "    Raises:\n",
    "    -------\n",
    "    FileNotFoundError\n",
    "        If the file doesn't exist\n",
    "    pd.errors.EmptyDataError\n",
    "        If the file is empty\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Loading {dataset_name} data\")\n",
    "        \n",
    "        if file_type == 'csv':\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_type == 'excel':\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported file type: {file_type}\")\n",
    "\n",
    "        print(f\"Successfully loaded: {df.shape[0]:,} rows Ã— {df.shape[1]:,} columns\")\n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\" Error: File not found - {file_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {dataset_name}: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c530ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Connectome data\n",
      "Successfully loaded: 1,213 rows Ã— 19,901 columns\n",
      "Loading Quantitative data\n",
      "Successfully loaded: 1,213 rows Ã— 19 columns\n",
      "Loading Categorical data\n",
      "Successfully loaded: 1,213 rows Ã— 10 columns\n",
      "Loading Target data\n",
      "Successfully loaded: 1,213 rows Ã— 3 columns\n",
      "\n",
      "ðŸ“ˆ Dataset Summary:\n",
      "Connectome      1213     19901    \n",
      "Quantitative    1213     19       \n",
      "Categorical     1213     10       \n",
      "Target          1213     3        \n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATASET LOADING \n",
    "# =============================================================================\n",
    "\n",
    "# Load Connectome Data (Functional brain connectivity matrices)\n",
    "conn_data = load_dataset(CONNECTOME_FILE, \"Connectome\", \"csv\")\n",
    "\n",
    "\n",
    "# Load Quantitative Metadata (Behavioral questionnaires and assessments)\n",
    "quant_data = load_dataset(QUANTITATIVE_FILE, \"Quantitative\", \"excel\")\n",
    "\n",
    "\n",
    "# Load Categorical Metadata (Demographics and categorical variables)\n",
    "cat_data = load_dataset(CATEGORICAL_FILE, \"Categorical\", \"excel\")\n",
    "\n",
    "\n",
    "# Load Target Data (ADHD outcome labels)\n",
    "target_data = load_dataset(TARGET_FILE, \"Target\", \"excel\")\n",
    "\n",
    "\n",
    "print(f\"\\n Dataset Summary:\")\n",
    "\n",
    "print(f\"{'Connectome':<15} {conn_data.shape[0]:<8} {conn_data.shape[1]:<8} \")\n",
    "print(f\"{'Quantitative':<15} {quant_data.shape[0]:<8} {quant_data.shape[1]:<8} \")\n",
    "print(f\"{'Categorical':<15} {cat_data.shape[0]:<8} {cat_data.shape[1]:<8} \")\n",
    "print(f\"{'Target':<15} {target_data.shape[0]:<8} {target_data.shape[1]:<8} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f1c7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Step 1: Added Quantitative\n",
      "            â€¢ Rows: 1,213 â†’ 1,213 (+0)\n",
      "            â€¢ Cols: 19,901 â†’ 19,919 (+18)\n",
      "   Step 2: Added Categorical\n",
      "            â€¢ Rows: 1,213 â†’ 1,213 (+0)\n",
      "            â€¢ Cols: 19,919 â†’ 19,928 (+9)\n",
      "   Step 3: Added Target\n",
      "            â€¢ Rows: 1,213 â†’ 1,213 (+0)\n",
      "            â€¢ Cols: 19,928 â†’ 19,930 (+2)\n",
      "\n",
      "ðŸŽ‰ Merging Complete!\n",
      "   Final dataset shape: 1,213 rows Ã— 19,930 columns\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA MERGING PROCESS\n",
    "# =============================================================================\n",
    "\n",
    "def merge_datasets_sequentially(datasets, dataset_names, key='participant_id'):\n",
    "    \"\"\"\n",
    "    Sequentially merge multiple datasets on a common key.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    datasets : list of pd.DataFrame\n",
    "        List of datasets to merge\n",
    "    dataset_names : list of str\n",
    "        Names of datasets for logging\n",
    "    merge_key : str\n",
    "        Column name to merge on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Final merged dataset\n",
    "    \n",
    "    Notes:\n",
    "    ------\n",
    "    Uses inner joins to ensure only participants with data in ALL datasets\n",
    "    are included in the final merged dataset.\n",
    "    \"\"\"\n",
    "  \n",
    "    \n",
    "    # Start with first dataset\n",
    "    merged_df = datasets[0].copy()\n",
    "\n",
    "    \n",
    "    #  merge remaining datasets\n",
    "    for i, (dataset, name) in enumerate(zip(datasets[1:], dataset_names[1:]), 1):\n",
    "        before_shape = merged_df.shape\n",
    "        \n",
    "        # Perform inner join to keep only matching participant_ids\n",
    "        merged_df = pd.merge(merged_df, dataset, on=key, how='inner')\n",
    "        after_shape = merged_df.shape\n",
    "        \n",
    "        # Calculate merge statistics\n",
    "        rows_lost = before_shape[0] - after_shape[0]\n",
    "        cols_added = after_shape[1] - before_shape[1]\n",
    "        \n",
    "        print(f\"   Step {i}: Added {name}\")\n",
    "        print(f\"            â€¢ Rows: {before_shape[0]:,} â†’ {after_shape[0]:,} ({-rows_lost:+,})\")\n",
    "        print(f\"            â€¢ Cols: {before_shape[1]:,} â†’ {after_shape[1]:,} ({cols_added:+,})\")\n",
    "        \n",
    "        \n",
    "    return merged_df\n",
    "\n",
    "# Perform sequential merging\n",
    "datasets = [conn_data, quant_data, cat_data, target_data]\n",
    "dataset_names = [\"Connectome\", \"Quantitative\", \"Categorical\", \"Target\"]\n",
    "\n",
    "merged_df = merge_datasets_sequentially(datasets, dataset_names)\n",
    "\n",
    "print(f\"Final dataset shape: {merged_df.shape[0]:,} rows Ã— {merged_df.shape[1]:,} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   âœ… Dataset saved: raw_dataset.csv\n",
      "   ðŸ“ File size: 446.5 MB\n",
      "\n",
      "ðŸŽ¯ DATA MERGING PROCESS COMPLETE!\n",
      "============================================================\n",
      "âœ… Successfully created unified ADHD dataset\n",
      "ðŸ“Š Final dataset: 1,213 participants Ã— 19,930 features\n",
      "ðŸ“ Output file: raw_dataset.csv\n",
      "ðŸš€ Ready for preprocessing pipeline!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# DATA EXPORT AND SUMMARY\n",
    "# =============================================================================\n",
    "\n",
    "def save_merged_dataset(df, output_path):\n",
    "    \"\"\"\n",
    "    Save merged dataset with metadata documentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Merged dataset to save\n",
    "    output_path : str or Path\n",
    "        Output file path\n",
    "    validation_report : dict\n",
    "        Validation statistics for documentation\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        # Save main dataset\n",
    "        df.to_csv(output_path, index=False)\n",
    "        file_size_mb = Path(output_path).stat().st_size / (1024*1024)\n",
    "        \n",
    "        print(f\"   âœ… Dataset saved: {output_path}\")\n",
    "        print(f\"   ðŸ“ File size: {file_size_mb:.1f} MB\")\n",
    "        \n",
    "        # Create metadata file\n",
    "        metadata = {\n",
    "            'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'source_files': {\n",
    "                'connectome': str(CONNECTOME_FILE),\n",
    "                'quantitative': str(QUANTITATIVE_FILE),\n",
    "                'categorical': str(CATEGORICAL_FILE),\n",
    "                'target': str(TARGET_FILE)\n",
    "            },\n",
    "            'merge_method': 'inner_join',\n",
    "            'merge_key': 'participant_id'\n",
    "        }\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = output_path.with_suffix('.json')\n",
    "        import json\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2, default=str)\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error saving dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Save the merged dataset\n",
    "save_merged_dataset(merged_df, OUTPUT_FILE)\n",
    "\n",
    "\n",
    "print(f\"ðŸ“Š Final dataset: {merged_df.shape[0]:,} participants Ã— {merged_df.shape[1]:,} features\")\n",
    "print(f\"ðŸ“ Output file: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80633141",
   "metadata": {},
   "source": [
    "DONE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
