{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd7e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\04ama\\Downloads\\raw adhd data\\raw_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e132ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove MRI_Track_Age_at_Scan\n",
    "if 'MRI_Track_Age_at_Scan' in df.columns:\n",
    "    df = df.drop(columns=['MRI_Track_Age_at_Scan'])\n",
    "\n",
    "# Identify quantitative and categorical columns (excluding target and participant_id)\n",
    "quant_cols = [col for col in df.columns if col.startswith('APQ_') or col.startswith('SDQ_') or col.startswith('EHQ_') or col.startswith('ColorVision')]\n",
    "cat_cols = [col for col in df.columns if col.startswith('PreInt_') or col.startswith('Basic_') or col.startswith('Sex_F')]\n",
    "conn_cols = list(df.iloc[:, 1:19902].columns)\n",
    "\n",
    "# Remove columns with high correlation (>=0.7)\n",
    "quant_df = df[quant_cols].copy()\n",
    "corr = quant_df.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.7)]\n",
    "df = df.drop(columns=to_drop)\n",
    "quant_cols = [col for col in quant_cols if col not in to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c17984e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>0throw_1thcolumn</th>\n",
       "      <th>0throw_2thcolumn</th>\n",
       "      <th>0throw_3thcolumn</th>\n",
       "      <th>0throw_4thcolumn</th>\n",
       "      <th>0throw_5thcolumn</th>\n",
       "      <th>0throw_6thcolumn</th>\n",
       "      <th>0throw_7thcolumn</th>\n",
       "      <th>0throw_8thcolumn</th>\n",
       "      <th>0throw_9thcolumn</th>\n",
       "      <th>...</th>\n",
       "      <th>Basic_Demos_Study_Site</th>\n",
       "      <th>PreInt_Demos_Fam_Child_Ethnicity</th>\n",
       "      <th>PreInt_Demos_Fam_Child_Race</th>\n",
       "      <th>MRI_Track_Scan_Location</th>\n",
       "      <th>Barratt_Barratt_P1_Edu</th>\n",
       "      <th>Barratt_Barratt_P1_Occ</th>\n",
       "      <th>Barratt_Barratt_P2_Edu</th>\n",
       "      <th>Barratt_Barratt_P2_Occ</th>\n",
       "      <th>ADHD_Outcome</th>\n",
       "      <th>Sex_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70z8Q2xdTXM3</td>\n",
       "      <td>0.222930</td>\n",
       "      <td>0.527903</td>\n",
       "      <td>0.429966</td>\n",
       "      <td>0.060457</td>\n",
       "      <td>0.566489</td>\n",
       "      <td>0.315342</td>\n",
       "      <td>0.508408</td>\n",
       "      <td>-0.078290</td>\n",
       "      <td>0.525692</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHWymJu6zNZi</td>\n",
       "      <td>0.614765</td>\n",
       "      <td>0.577255</td>\n",
       "      <td>0.496127</td>\n",
       "      <td>0.496606</td>\n",
       "      <td>0.404686</td>\n",
       "      <td>0.439724</td>\n",
       "      <td>0.122590</td>\n",
       "      <td>-0.085452</td>\n",
       "      <td>0.120673</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4PAQp1M6EyAo</td>\n",
       "      <td>-0.116833</td>\n",
       "      <td>0.458408</td>\n",
       "      <td>0.260703</td>\n",
       "      <td>0.639031</td>\n",
       "      <td>0.769337</td>\n",
       "      <td>0.442528</td>\n",
       "      <td>0.637110</td>\n",
       "      <td>0.192010</td>\n",
       "      <td>0.520379</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>obEacy4Of68I</td>\n",
       "      <td>0.199688</td>\n",
       "      <td>0.752714</td>\n",
       "      <td>0.658283</td>\n",
       "      <td>0.575096</td>\n",
       "      <td>0.692867</td>\n",
       "      <td>0.645789</td>\n",
       "      <td>0.522750</td>\n",
       "      <td>0.412188</td>\n",
       "      <td>0.530843</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s7WzzDcmDOhF</td>\n",
       "      <td>0.227321</td>\n",
       "      <td>0.613268</td>\n",
       "      <td>0.621447</td>\n",
       "      <td>0.562673</td>\n",
       "      <td>0.736709</td>\n",
       "      <td>0.589813</td>\n",
       "      <td>0.266676</td>\n",
       "      <td>0.359668</td>\n",
       "      <td>0.300771</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 19923 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id  0throw_1thcolumn  0throw_2thcolumn  0throw_3thcolumn  \\\n",
       "0   70z8Q2xdTXM3          0.222930          0.527903          0.429966   \n",
       "1   WHWymJu6zNZi          0.614765          0.577255          0.496127   \n",
       "2   4PAQp1M6EyAo         -0.116833          0.458408          0.260703   \n",
       "3   obEacy4Of68I          0.199688          0.752714          0.658283   \n",
       "4   s7WzzDcmDOhF          0.227321          0.613268          0.621447   \n",
       "\n",
       "   0throw_4thcolumn  0throw_5thcolumn  0throw_6thcolumn  0throw_7thcolumn  \\\n",
       "0          0.060457          0.566489          0.315342          0.508408   \n",
       "1          0.496606          0.404686          0.439724          0.122590   \n",
       "2          0.639031          0.769337          0.442528          0.637110   \n",
       "3          0.575096          0.692867          0.645789          0.522750   \n",
       "4          0.562673          0.736709          0.589813          0.266676   \n",
       "\n",
       "   0throw_8thcolumn  0throw_9thcolumn  ...  Basic_Demos_Study_Site  \\\n",
       "0         -0.078290          0.525692  ...                       1   \n",
       "1         -0.085452          0.120673  ...                       1   \n",
       "2          0.192010          0.520379  ...                       1   \n",
       "3          0.412188          0.530843  ...                       1   \n",
       "4          0.359668          0.300771  ...                       1   \n",
       "\n",
       "   PreInt_Demos_Fam_Child_Ethnicity  PreInt_Demos_Fam_Child_Race  \\\n",
       "0                               0.0                          1.0   \n",
       "1                               1.0                          8.0   \n",
       "2                               0.0                          0.0   \n",
       "3                               0.0                          0.0   \n",
       "4                               2.0                          8.0   \n",
       "\n",
       "   MRI_Track_Scan_Location  Barratt_Barratt_P1_Edu  Barratt_Barratt_P1_Occ  \\\n",
       "0                      2.0                    21.0                    45.0   \n",
       "1                      1.0                     6.0                     5.0   \n",
       "2                      2.0                    18.0                    35.0   \n",
       "3                      2.0                    21.0                    40.0   \n",
       "4                      2.0                     9.0                    35.0   \n",
       "\n",
       "   Barratt_Barratt_P2_Edu  Barratt_Barratt_P2_Occ  ADHD_Outcome  Sex_F  \n",
       "0                    21.0                    45.0             1      0  \n",
       "1                     NaN                    15.0             1      1  \n",
       "2                     9.0                    20.0             1      1  \n",
       "3                    21.0                    40.0             1      1  \n",
       "4                     NaN                     NaN             1      1  \n",
       "\n",
       "[5 rows x 19923 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd2ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('quant_cols.json', 'w') as f:\n",
    "    json.dump(quant_cols, f)\n",
    "with open('cat_cols.json', 'w') as f:\n",
    "    json.dump(cat_cols, f)\n",
    "with open('conn_cols.json', 'w') as f:\n",
    "    json.dump(conn_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. AFTER IMPUTATION:\n",
      "Quant cols NaN: 0\n",
      "Cat cols NaN: 0\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: SPLIT DATA FIRST (BEFORE ANY PREPROCESSING)\n",
    "print(\"ðŸ”„ SPLITTING DATA BEFORE PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "target_col = 'ADHD_Outcome'\n",
    "X = df.drop(columns=[target_col, 'participant_id'], errors='ignore')\n",
    "y = df[target_col]\n",
    "\n",
    "# First split: train+val vs test (80% vs 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: train vs val (60% vs 20% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Check original distributions\n",
    "print(f\"\\nOriginal distributions:\")\n",
    "print(f\"Training: {dict(Counter(y_train))}\")\n",
    "print(f\"Validation: {dict(Counter(y_val))}\")\n",
    "print(f\"Test: {dict(Counter(y_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b960f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: APPLY PREPROCESSING TO ALL SPLITS\n",
    "print(\"\\nðŸ”§ APPLYING PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 2.1 Impute missing values (fit on training, transform all)\n",
    "print(\"Imputing missing values...\")\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit on training data only\n",
    "imputer.fit(X_train[quant_cols + cat_cols])\n",
    "\n",
    "# Transform all splits\n",
    "X_train[quant_cols + cat_cols] = imputer.transform(X_train[quant_cols + cat_cols])\n",
    "X_val[quant_cols + cat_cols] = imputer.transform(X_val[quant_cols + cat_cols])\n",
    "X_test[quant_cols + cat_cols] = imputer.transform(X_test[quant_cols + cat_cols])\n",
    "\n",
    "joblib.dump(imputer, 'imputer.joblib')\n",
    "print(\"âœ… Imputation complete\")\n",
    "\n",
    "# 2.2 Scale quantitative columns (fit on training, transform all)\n",
    "print(\"Scaling quantitative features...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "scaler.fit(X_train[quant_cols])\n",
    "\n",
    "# Transform all splits\n",
    "X_train[quant_cols] = scaler.transform(X_train[quant_cols])\n",
    "X_val[quant_cols] = scaler.transform(X_val[quant_cols])\n",
    "X_test[quant_cols] = scaler.transform(X_test[quant_cols])\n",
    "\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"âœ… Scaling complete\")\n",
    "\n",
    "# 2.3 One-hot encode categorical columns (fit on training, transform all)\n",
    "print(\"Encoding categorical features...\")\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit on training data only\n",
    "encoder.fit(X_train[cat_cols])\n",
    "\n",
    "# Transform all splits\n",
    "encoded_train = encoder.transform(X_train[cat_cols])\n",
    "encoded_val = encoder.transform(X_val[cat_cols])\n",
    "encoded_test = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "# Create encoded dataframes\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(cat_cols), index=X_train.index)\n",
    "encoded_val_df = pd.DataFrame(encoded_val, columns=encoder.get_feature_names_out(cat_cols), index=X_val.index)\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(cat_cols), index=X_test.index)\n",
    "\n",
    "# Drop original categorical columns and add encoded ones\n",
    "X_train = X_train.drop(columns=cat_cols)\n",
    "X_val = X_val.drop(columns=cat_cols)\n",
    "X_test = X_test.drop(columns=cat_cols)\n",
    "\n",
    "X_train = pd.concat([X_train, encoded_train_df], axis=1)\n",
    "X_val = pd.concat([X_val, encoded_val_df], axis=1)\n",
    "X_test = pd.concat([X_test, encoded_test_df], axis=1)\n",
    "\n",
    "joblib.dump(encoder, 'encoder.joblib')\n",
    "print(\"âœ… Encoding complete\")\n",
    "\n",
    "# 2.4 PCA for connectome (fit on training, transform all)\n",
    "if len(conn_cols) > 0:\n",
    "    print(\"Applying PCA to connectome features...\")\n",
    "    pca = PCA(n_components=10)\n",
    "    \n",
    "    # Fit on training data only\n",
    "    pca.fit(X_train[conn_cols])\n",
    "    \n",
    "    # Transform all splits\n",
    "    conn_pca_train = pca.transform(X_train[conn_cols])\n",
    "    conn_pca_val = pca.transform(X_val[conn_cols])\n",
    "    conn_pca_test = pca.transform(X_test[conn_cols])\n",
    "    \n",
    "    # Create PCA dataframes\n",
    "    pca_cols = [f'conn_pca_{i+1}' for i in range(10)]\n",
    "    conn_pca_train_df = pd.DataFrame(conn_pca_train, columns=pca_cols, index=X_train.index)\n",
    "    conn_pca_val_df = pd.DataFrame(conn_pca_val, columns=pca_cols, index=X_val.index)\n",
    "    conn_pca_test_df = pd.DataFrame(conn_pca_test, columns=pca_cols, index=X_test.index)\n",
    "    \n",
    "    # Drop original connectome columns and add PCA components\n",
    "    X_train = X_train.drop(columns=conn_cols)\n",
    "    X_val = X_val.drop(columns=conn_cols)\n",
    "    X_test = X_test.drop(columns=conn_cols)\n",
    "    \n",
    "    X_train = pd.concat([X_train, conn_pca_train_df], axis=1)\n",
    "    X_val = pd.concat([X_val, conn_pca_val_df], axis=1)\n",
    "    X_test = pd.concat([X_test, conn_pca_test_df], axis=1)\n",
    "    \n",
    "    joblib.dump(pca, 'pca_connectome.joblib')\n",
    "    print(\"âœ… PCA complete\")\n",
    "\n",
    "print(f\"\\nPreprocessing complete:\")\n",
    "print(f\"Training features: {X_train.shape[1]}\")\n",
    "print(f\"Validation features: {X_val.shape[1]}\")\n",
    "print(f\"Test features: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecc427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final NaN check: 469\n",
      "Filling remaining NaN values...\n",
      "NaN after final cleanup: 0\n"
     ]
    }
   ],
   "source": [
    "# Final NaN check and cleanup for all splits\n",
    "print(\"\\nðŸ§¹ FINAL CLEANUP\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for name, X_split in [('Training', X_train), ('Validation', X_val), ('Test', X_test)]:\n",
    "    nan_count = X_split.isnull().sum().sum()\n",
    "    print(f\"{name} NaN count: {nan_count}\")\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f\"Cleaning {name} set...\")\n",
    "        # Fill numeric columns with median, others with 0\n",
    "        numeric_cols = X_split.select_dtypes(include=[np.number]).columns\n",
    "        X_split[numeric_cols] = X_split[numeric_cols].fillna(X_split[numeric_cols].median())\n",
    "        X_split = X_split.fillna(0)  # Fill any remaining non-numeric NaN\n",
    "        \n",
    "        if name == 'Training':\n",
    "            X_train = X_split\n",
    "        elif name == 'Validation':\n",
    "            X_val = X_split\n",
    "        else:\n",
    "            X_test = X_split\n",
    "\n",
    "print(\"âœ… All splits cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: APPLY ADASYN ONLY TO TRAINING DATA\n",
    "print(\"\\nðŸŽ¯ APPLYING ADASYN TO TRAINING DATA ONLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check original training distribution\n",
    "original_counts = Counter(y_train)\n",
    "original_ratio = max(original_counts.values()) / min(original_counts.values())\n",
    "print(f\"Original training distribution: {dict(original_counts)}\")\n",
    "print(f\"Original imbalance ratio: {original_ratio:.3f}:1\")\n",
    "\n",
    "# Apply ADASYN with specified parameters\n",
    "adasyn = ADASYN(n_neighbors=15, random_state=42, sampling_strategy=0.7)\n",
    "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check new training distribution\n",
    "new_counts = Counter(y_train_balanced)\n",
    "new_ratio = max(new_counts.values()) / min(new_counts.values())\n",
    "print(f\"\\nADASYN applied successfully:\")\n",
    "print(f\"New training distribution: {dict(new_counts)}\")\n",
    "print(f\"New imbalance ratio: {new_ratio:.3f}:1\")\n",
    "print(f\"Samples added: {len(X_train_balanced) - len(X_train):,}\")\n",
    "print(f\"Balance improvement: {original_ratio - new_ratio:.3f}\")\n",
    "\n",
    "# Validation and test sets remain unchanged (no data leakage!)\n",
    "print(f\"\\nâœ… DATA PREPARATION COMPLETE (NO DATA LEAKAGE)\")\n",
    "print(f\"Training (balanced): {X_train_balanced.shape}\")\n",
    "print(f\"Validation (original): {X_val.shape}\")  \n",
    "print(f\"Test (original): {X_test.shape}\")\n",
    "\n",
    "# Update training data to use balanced version\n",
    "X_train = X_train_balanced\n",
    "y_train = y_train_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba39a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: TRAIN MODEL WITH BALANCED TRAINING DATA\n",
    "print(\"\\nðŸ¤– TRAINING MODEL\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Train logistic regression with optimized parameters\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    solver='liblinear', \n",
    "    C=0.1\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"âœ… Model training complete\")\n",
    "\n",
    "# Evaluate on test set with custom threshold\n",
    "y_test_proba = lr.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.45\n",
    "y_test_pred_custom = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"\\nðŸ“Š TEST SET EVALUATION (threshold={threshold})\")\n",
    "print(\"=\" * 50)\n",
    "report = classification_report(y_test, y_test_pred_custom)\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred_custom)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_test_pred_custom, average='macro')\n",
    "print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "\n",
    "# Save model and threshold\n",
    "joblib.dump(lr, 'adhd_logistic_model.joblib')\n",
    "\n",
    "with open('adhd_lr_threshold.json', 'w') as f:\n",
    "    json.dump({'threshold': threshold}, f)\n",
    "\n",
    "print(f\"\\nâœ… Model and threshold saved\")\n",
    "print(f\"Model file: adhd_logistic_model.joblib\")\n",
    "print(f\"Threshold file: adhd_lr_threshold.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
