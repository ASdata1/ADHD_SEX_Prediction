{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e4cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\04ama\\Downloads\\raw adhd data\\raw_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e132ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Remove MRI_Track_Age_at_Scan\n",
    "if 'MRI_Track_Age_at_Scan' in df.columns:\n",
    "    df = df.drop(columns=['MRI_Track_Age_at_Scan'])\n",
    "\n",
    "# Identify quantitative and categorical columns (excluding target and participant_id)\n",
    "quant_cols = [col for col in df.columns if col.startswith('APQ_') or col.startswith('SDQ_') or col.startswith('EHQ_') or col.startswith('ColorVision')]\n",
    "cat_cols = [col for col in df.columns if col.startswith('PreInt_') or col.startswith('Basic_') or col.startswith('Sex_F')]\n",
    "conn_cols = list(df.iloc[:, 1:19902].columns)\n",
    "\n",
    "# Remove columns with high correlation (>=0.7)\n",
    "quant_df = df[quant_cols].copy()\n",
    "corr = quant_df.corr().abs()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >= 0.7)]\n",
    "df = df.drop(columns=to_drop)\n",
    "quant_cols = [col for col in quant_cols if col not in to_drop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('quant_cols.json', 'w') as f:\n",
    "    json.dump(quant_cols, f)\n",
    "with open('cat_cols.json', 'w') as f:\n",
    "    json.dump(cat_cols, f)\n",
    "with open('conn_cols.json', 'w') as f:\n",
    "    json.dump(conn_cols, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: SPLIT DATA FIRST (BEFORE ANY PREPROCESSING)\n",
    "print(\"ðŸ”„ SPLITTING DATA BEFORE PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "target_col = 'ADHD_Outcome'\n",
    "X = df.drop(columns=[target_col, 'participant_id'], errors='ignore')\n",
    "y = df[target_col]\n",
    "\n",
    "# First split: train+val vs test (80% vs 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Second split: train vs val (60% vs 20% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X):.1%})\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X):.1%})\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X):.1%})\")\n",
    "\n",
    "# Check original distributions\n",
    "print(f\"\\nOriginal distributions:\")\n",
    "print(f\"Training: {dict(Counter(y_train))}\")\n",
    "print(f\"Validation: {dict(Counter(y_val))}\")\n",
    "print(f\"Test: {dict(Counter(y_test))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b960f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: APPLY PREPROCESSING TO ALL SPLITS\n",
    "print(\"\\nðŸ”§ APPLYING PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 2.1 Impute missing values (fit on training, transform all)\n",
    "print(\"Imputing missing values...\")\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Fit on training data only\n",
    "imputer.fit(X_train[quant_cols + cat_cols])\n",
    "\n",
    "# Transform all splits\n",
    "X_train[quant_cols + cat_cols] = imputer.transform(X_train[quant_cols + cat_cols])\n",
    "X_val[quant_cols + cat_cols] = imputer.transform(X_val[quant_cols + cat_cols])\n",
    "X_test[quant_cols + cat_cols] = imputer.transform(X_test[quant_cols + cat_cols])\n",
    "\n",
    "joblib.dump(imputer, 'imputer.joblib')\n",
    "print(\"âœ… Imputation complete\")\n",
    "\n",
    "# 2.2 Scale quantitative columns (fit on training, transform all)\n",
    "print(\"Scaling quantitative features...\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit on training data only\n",
    "scaler.fit(X_train[quant_cols])\n",
    "\n",
    "# Transform all splits\n",
    "X_train[quant_cols] = scaler.transform(X_train[quant_cols])\n",
    "X_val[quant_cols] = scaler.transform(X_val[quant_cols])\n",
    "X_test[quant_cols] = scaler.transform(X_test[quant_cols])\n",
    "\n",
    "joblib.dump(scaler, 'scaler.joblib')\n",
    "print(\"âœ… Scaling complete\")\n",
    "\n",
    "# 2.3 One-hot encode categorical columns (fit on training, transform all)\n",
    "print(\"Encoding categorical features...\")\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit on training data only\n",
    "encoder.fit(X_train[cat_cols])\n",
    "\n",
    "# Transform all splits\n",
    "encoded_train = encoder.transform(X_train[cat_cols])\n",
    "encoded_val = encoder.transform(X_val[cat_cols])\n",
    "encoded_test = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "# Create encoded dataframes\n",
    "encoded_train_df = pd.DataFrame(encoded_train, columns=encoder.get_feature_names_out(cat_cols), index=X_train.index)\n",
    "encoded_val_df = pd.DataFrame(encoded_val, columns=encoder.get_feature_names_out(cat_cols), index=X_val.index)\n",
    "encoded_test_df = pd.DataFrame(encoded_test, columns=encoder.get_feature_names_out(cat_cols), index=X_test.index)\n",
    "\n",
    "# Drop original categorical columns and add encoded ones\n",
    "X_train = X_train.drop(columns=cat_cols)\n",
    "X_val = X_val.drop(columns=cat_cols)\n",
    "X_test = X_test.drop(columns=cat_cols)\n",
    "\n",
    "X_train = pd.concat([X_train, encoded_train_df], axis=1)\n",
    "X_val = pd.concat([X_val, encoded_val_df], axis=1)\n",
    "X_test = pd.concat([X_test, encoded_test_df], axis=1)\n",
    "\n",
    "joblib.dump(encoder, 'encoder.joblib')\n",
    "print(\"âœ… Encoding complete\")\n",
    "\n",
    "# 2.4 PCA for connectome (fit on training, transform all)\n",
    "if len(conn_cols) > 0:\n",
    "    print(\"Applying PCA to connectome features...\")\n",
    "    pca = PCA(n_components=10)\n",
    "    \n",
    "    # Fit on training data only\n",
    "    pca.fit(X_train[conn_cols])\n",
    "    \n",
    "    # Transform all splits\n",
    "    conn_pca_train = pca.transform(X_train[conn_cols])\n",
    "    conn_pca_val = pca.transform(X_val[conn_cols])\n",
    "    conn_pca_test = pca.transform(X_test[conn_cols])\n",
    "    \n",
    "    # Create PCA dataframes\n",
    "    pca_cols = [f'conn_pca_{i+1}' for i in range(10)]\n",
    "    conn_pca_train_df = pd.DataFrame(conn_pca_train, columns=pca_cols, index=X_train.index)\n",
    "    conn_pca_val_df = pd.DataFrame(conn_pca_val, columns=pca_cols, index=X_val.index)\n",
    "    conn_pca_test_df = pd.DataFrame(conn_pca_test, columns=pca_cols, index=X_test.index)\n",
    "    \n",
    "    # Drop original connectome columns and add PCA components\n",
    "    X_train = X_train.drop(columns=conn_cols)\n",
    "    X_val = X_val.drop(columns=conn_cols)\n",
    "    X_test = X_test.drop(columns=conn_cols)\n",
    "    \n",
    "    X_train = pd.concat([X_train, conn_pca_train_df], axis=1)\n",
    "    X_val = pd.concat([X_val, conn_pca_val_df], axis=1)\n",
    "    X_test = pd.concat([X_test, conn_pca_test_df], axis=1)\n",
    "    \n",
    "    joblib.dump(pca, 'pca_connectome.joblib')\n",
    "    print(\"âœ… PCA complete\")\n",
    "\n",
    "print(f\"\\nPreprocessing complete:\")\n",
    "print(f\"Training features: {X_train.shape[1]}\")\n",
    "print(f\"Validation features: {X_val.shape[1]}\")\n",
    "print(f\"Test features: {X_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ecc427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final NaN check and cleanup for all splits\n",
    "print(\"\\nðŸ§¹ FINAL CLEANUP\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "for name, X_split in [('Training', X_train), ('Validation', X_val), ('Test', X_test)]:\n",
    "    nan_count = X_split.isnull().sum().sum()\n",
    "    print(f\"{name} NaN count: {nan_count}\")\n",
    "    \n",
    "    if nan_count > 0:\n",
    "        print(f\"Cleaning {name} set...\")\n",
    "        # Fill numeric columns with median, others with 0\n",
    "        numeric_cols = X_split.select_dtypes(include=[np.number]).columns\n",
    "        X_split[numeric_cols] = X_split[numeric_cols].fillna(X_split[numeric_cols].median())\n",
    "        X_split = X_split.fillna(0)  # Fill any remaining non-numeric NaN\n",
    "        \n",
    "        if name == 'Training':\n",
    "            X_train = X_split\n",
    "        elif name == 'Validation':\n",
    "            X_val = X_split\n",
    "        else:\n",
    "            X_test = X_split\n",
    "\n",
    "print(\"âœ… All splits cleaned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: APPLY ADASYN ONLY TO TRAINING DATA\n",
    "print(\"\\nðŸŽ¯ APPLYING ADASYN TO TRAINING DATA ONLY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check original training distribution\n",
    "original_counts = Counter(y_train)\n",
    "original_ratio = max(original_counts.values()) / min(original_counts.values())\n",
    "print(f\"Original training distribution: {dict(original_counts)}\")\n",
    "print(f\"Original imbalance ratio: {original_ratio:.3f}:1\")\n",
    "\n",
    "# Apply ADASYN with specified parameters\n",
    "adasyn = ADASYN(n_neighbors=15, random_state=42, sampling_strategy=0.7)\n",
    "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "# Check new training distribution\n",
    "new_counts = Counter(y_train_balanced)\n",
    "new_ratio = max(new_counts.values()) / min(new_counts.values())\n",
    "print(f\"\\nADASYN applied successfully:\")\n",
    "print(f\"New training distribution: {dict(new_counts)}\")\n",
    "print(f\"New imbalance ratio: {new_ratio:.3f}:1\")\n",
    "print(f\"Samples added: {len(X_train_balanced) - len(X_train):,}\")\n",
    "print(f\"Balance improvement: {original_ratio - new_ratio:.3f}\")\n",
    "\n",
    "# Validation and test sets remain unchanged (no data leakage!)\n",
    "print(f\"\\nâœ… DATA PREPARATION COMPLETE (NO DATA LEAKAGE)\")\n",
    "print(f\"Training (balanced): {X_train_balanced.shape}\")\n",
    "print(f\"Validation (original): {X_val.shape}\")  \n",
    "print(f\"Test (original): {X_test.shape}\")\n",
    "\n",
    "# Update training data to use balanced version\n",
    "X_train = X_train_balanced\n",
    "y_train = y_train_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba39a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: TRAIN MODEL WITH BALANCED TRAINING DATA\n",
    "print(\"\\nðŸ¤– TRAINING MODEL\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Train logistic regression with optimized parameters\n",
    "lr = LogisticRegression(\n",
    "    max_iter=1000, \n",
    "    random_state=42, \n",
    "    class_weight='balanced', \n",
    "    solver='liblinear', \n",
    "    C=0.1\n",
    ")\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"âœ… Model training complete\")\n",
    "\n",
    "# Evaluate on test set with custom threshold\n",
    "y_test_proba = lr.predict_proba(X_test)[:, 1]\n",
    "threshold = 0.50\n",
    "y_test_pred_custom = (y_test_proba >= threshold).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(f\"\\nðŸ“Š TEST SET EVALUATION (threshold={threshold})\")\n",
    "print(\"=\" * 50)\n",
    "report = classification_report(y_test, y_test_pred_custom)\n",
    "print(report)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_test_pred_custom)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "macro_f1 = f1_score(y_test, y_test_pred_custom, average='macro')\n",
    "print(f\"Macro F1-score: {macro_f1:.4f}\")\n",
    "\n",
    "# Save model and threshold\n",
    "joblib.dump(lr, 'adhd_logistic_model.joblib')\n",
    "\n",
    "with open('adhd_lr_threshold.json', 'w') as f:\n",
    "    json.dump({'threshold': threshold}, f)\n",
    "\n",
    "print(f\"\\nâœ… Model and threshold saved\")\n",
    "print(f\"Model file: adhd_logistic_model.joblib\")\n",
    "print(f\"Threshold file: adhd_lr_threshold.json\")\n",
    "print('hello')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
