{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e67ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb86346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (1213, 19930)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\04ama\\Downloads\\raw adhd data\\raw_dataset.csv\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c9240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Preparing Data for ADASYN\n",
      "ðŸ“Š Original imbalance ratio: 2.175:1\n",
      "\n",
      "ðŸŽ¯ Applying ADASYN...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nADASYN does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸŽ¯ Applying ADASYN...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m adasyn \u001b[38;5;241m=\u001b[39m ADASYN(n_neighbors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, sampling_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m X_balanced, y_balanced \u001b[38;5;241m=\u001b[39m adasyn\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate new imbalance ratio\u001b[39;00m\n\u001b[0;32m     24\u001b[0m new_counts \u001b[38;5;241m=\u001b[39m Counter(y_balanced)\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:106\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    104\u001b[0m check_classification_targets(y)\n\u001b[0;32m    105\u001b[0m arrays_transformer \u001b[38;5;241m=\u001b[39m ArraysTransformer(X, y)\n\u001b[1;32m--> 106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_resample(X, y)\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\imblearn\\base.py:161\u001b[0m, in \u001b[0;36mBaseSampler._check_X_y\u001b[1;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[0;32m    159\u001b[0m     accept_sparse \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    160\u001b[0m y, binarize_y \u001b[38;5;241m=\u001b[39m check_target_type(y, indicate_one_vs_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y, binarize_y\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1146\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1141\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1144\u001b[0m     )\n\u001b[1;32m-> 1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1149\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1150\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1151\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1152\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1153\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1154\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1155\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1156\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1157\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1158\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1164\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    954\u001b[0m         )\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 957\u001b[0m         _assert_all_finite(\n\u001b[0;32m    958\u001b[0m             array,\n\u001b[0;32m    959\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    960\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    961\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    962\u001b[0m         )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    123\u001b[0m     X,\n\u001b[0;32m    124\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    125\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    126\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    127\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    128\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    129\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\04ama\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    170\u001b[0m     )\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nADASYN does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6a2e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Preparing Data for ADASYN\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ”„ Preparing Data for ADASYN\")\n",
    "X = df.drop(columns=['ADHD_Outcome', 'participant_id'], errors='ignore')\n",
    "y = df['ADHD_Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e60d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cols = [col for col in df.columns if col.startswith('APQ_') or col.startswith('SDQ_') or col.startswith('EHQ_') or col.startswith('ColorVision')]\n",
    "cat_cols = [col for col in df.columns if col.startswith('PreInt_') or col.startswith('Basic_') or col.startswith('Handedness') or col.startswith('Sex_F')]\n",
    "conn_cols = list(df.iloc[:, 1:19902].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6141a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d841f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac8efe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = X_train.copy()\n",
    "X_val_processed = X_val.copy()\n",
    "X_test_processed = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e6ce016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling quantitative features\n"
     ]
    }
   ],
   "source": [
    "print(\"Scaling quantitative features\")\n",
    "X_train_processed[quant_cols] = scaler.fit_transform(X_train_processed[quant_cols])\n",
    "X_val_processed[quant_cols] = scaler.transform(X_val_processed[quant_cols])\n",
    "X_test_processed[quant_cols] = scaler.transform(X_test_processed[quant_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c40472",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_cols = quant_cols + cat_cols\n",
    "X_train_processed[all_feature_cols] = imputer.fit_transform(X_train_processed[all_feature_cols])\n",
    "X_val_processed[all_feature_cols] = imputer.transform(X_val_processed[all_feature_cols])\n",
    "X_test_processed[all_feature_cols] = imputer.transform(X_test_processed[all_feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0384a2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Original training imbalance:\n",
      "   â€¢ Original imbalance ratio: 2.175:1\n",
      "\n",
      "ðŸŽ¯ Applying ADASYN to training data...\n",
      "   â€¢ New imbalance ratio: 1.004:1\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# SIMPLE ADASYN IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare data for ADASYN\n",
    "\n",
    "\n",
    "# Calculate original imbalance ratio\n",
    "print(\"\\nðŸ“Š Original training imbalance:\")\n",
    "original_counts = Counter(y_train)\n",
    "original_ratio = max(original_counts.values()) / min(original_counts.values())\n",
    "print(f\"   â€¢ Original imbalance ratio: {original_ratio:.3f}:1\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Applying ADASYN to training data...\")\n",
    "adasyn = ADASYN(n_neighbors=15, random_state=42, sampling_strategy='auto')\n",
    "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train_processed[all_feature_cols], y_train)\n",
    "\n",
    "# Calculate new imbalance ratio\n",
    "new_counts = Counter(y_train_balanced)\n",
    "new_ratio = max(new_counts.values()) / min(new_counts.values())\n",
    "print(f\"   â€¢ New imbalance ratio: {new_ratio:.3f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96f7244a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING SFS WITH 5 FEATURES\n",
      "Running forward selection on 22 features...\n",
      "Target: Select 5 best features\n",
      "\n",
      "SFS completed in 3.8 seconds\n",
      "Selected 5 features:\n",
      "  - Quantitative: 4\n",
      "  - Categorical: 1\n",
      "\n",
      "Top selected features:\n",
      "   1. ColorVision_CV_Score (Quantitative)\n",
      "   2. APQ_P_APQ_P_OPD (Quantitative)\n",
      "   3. SDQ_SDQ_Emotional_Problems (Quantitative)\n",
      "   4. SDQ_SDQ_Hyperactivity (Quantitative)\n",
      "   5. Basic_Demos_Study_Site (Categorical)\n",
      "\n",
      "--- Cross-Validation Evaluation ---\n",
      "Cross-validation F1-Macro scores: [0.74822479 0.72918233 0.76881378 0.77105263 0.78631579]\n",
      "Mean F1-Macro: 0.7607\n",
      "Median F1-Macro: 0.7688\n",
      "Std Dev F1-Macro: 0.0199\n",
      "\n",
      "Validation F1-Macro: 0.7562\n",
      "Test F1-Macro: 0.7139\n",
      "Test AUC: 0.8037\n",
      "Test Accuracy: 0.7325\n",
      "ADHD Precision: 0.8633\n",
      "ADHD Recall: 0.7229\n",
      "TESTING SFS WITH 10 FEATURES\n",
      "Running forward selection on 22 features...\n",
      "Target: Select 10 best features\n",
      "\n",
      "SFS completed in 8.0 seconds\n",
      "Selected 10 features:\n",
      "  - Quantitative: 7\n",
      "  - Categorical: 3\n",
      "\n",
      "Top selected features:\n",
      "   1. ColorVision_CV_Score (Quantitative)\n",
      "   2. APQ_P_APQ_P_CP (Quantitative)\n",
      "   3. APQ_P_APQ_P_ID (Quantitative)\n",
      "   4. APQ_P_APQ_P_OPD (Quantitative)\n",
      "   5. SDQ_SDQ_Emotional_Problems (Quantitative)\n",
      "   6. SDQ_SDQ_Hyperactivity (Quantitative)\n",
      "   7. SDQ_SDQ_Prosocial (Quantitative)\n",
      "   8. Basic_Demos_Enroll_Year (Categorical)\n",
      "   9. Basic_Demos_Study_Site (Categorical)\n",
      "  10. PreInt_Demos_Fam_Child_Ethnicity (Categorical)\n",
      "\n",
      "--- Cross-Validation Evaluation ---\n",
      "Cross-validation F1-Macro scores: [0.74822479 0.75020129 0.75803087 0.77105263 0.79490806]\n",
      "Mean F1-Macro: 0.7645\n",
      "Median F1-Macro: 0.7580\n",
      "Std Dev F1-Macro: 0.0172\n",
      "\n",
      "Validation F1-Macro: 0.7738\n",
      "Test F1-Macro: 0.6976\n",
      "Test AUC: 0.7990\n",
      "Test Accuracy: 0.7160\n",
      "ADHD Precision: 0.8540\n",
      "ADHD Recall: 0.7048\n",
      "TESTING SFS WITH 15 FEATURES\n",
      "Running forward selection on 22 features...\n",
      "Target: Select 15 best features\n",
      "\n",
      "SFS completed in 9.6 seconds\n",
      "Selected 15 features:\n",
      "  - Quantitative: 11\n",
      "  - Categorical: 4\n",
      "\n",
      "Top selected features:\n",
      "   1. ColorVision_CV_Score (Quantitative)\n",
      "   2. APQ_P_APQ_P_CP (Quantitative)\n",
      "   3. APQ_P_APQ_P_ID (Quantitative)\n",
      "   4. APQ_P_APQ_P_INV (Quantitative)\n",
      "   5. APQ_P_APQ_P_OPD (Quantitative)\n",
      "   6. APQ_P_APQ_P_PP (Quantitative)\n",
      "   7. SDQ_SDQ_Conduct_Problems (Quantitative)\n",
      "   8. SDQ_SDQ_Emotional_Problems (Quantitative)\n",
      "   9. SDQ_SDQ_Externalizing (Quantitative)\n",
      "  10. SDQ_SDQ_Hyperactivity (Quantitative)\n",
      "  11. SDQ_SDQ_Prosocial (Quantitative)\n",
      "  12. Basic_Demos_Enroll_Year (Categorical)\n",
      "  13. Basic_Demos_Study_Site (Categorical)\n",
      "  14. PreInt_Demos_Fam_Child_Ethnicity (Categorical)\n",
      "  15. Sex_F (Categorical)\n",
      "      ... and 5 more\n",
      "\n",
      "--- Cross-Validation Evaluation ---\n",
      "Cross-validation F1-Macro scores: [0.73341449 0.75020129 0.75803087 0.77105263 0.79673953]\n",
      "Mean F1-Macro: 0.7619\n",
      "Median F1-Macro: 0.7580\n",
      "Std Dev F1-Macro: 0.0213\n",
      "\n",
      "Validation F1-Macro: 0.7530\n",
      "Test F1-Macro: 0.7112\n",
      "Test AUC: 0.7988\n",
      "Test Accuracy: 0.7325\n",
      "ADHD Precision: 0.8531\n",
      "ADHD Recall: 0.7349\n",
      "TESTING SFS WITH 20 FEATURES\n",
      "Running forward selection on 22 features...\n",
      "Target: Select 20 best features\n",
      "\n",
      "SFS completed in 13.0 seconds\n",
      "Selected 20 features:\n",
      "  - Quantitative: 16\n",
      "  - Categorical: 4\n",
      "\n",
      "Top selected features:\n",
      "   1. ColorVision_CV_Score (Quantitative)\n",
      "   2. APQ_P_APQ_P_CP (Quantitative)\n",
      "   3. APQ_P_APQ_P_ID (Quantitative)\n",
      "   4. APQ_P_APQ_P_INV (Quantitative)\n",
      "   5. APQ_P_APQ_P_OPD (Quantitative)\n",
      "   6. APQ_P_APQ_P_PM (Quantitative)\n",
      "   7. APQ_P_APQ_P_PP (Quantitative)\n",
      "   8. SDQ_SDQ_Conduct_Problems (Quantitative)\n",
      "   9. SDQ_SDQ_Difficulties_Total (Quantitative)\n",
      "  10. SDQ_SDQ_Emotional_Problems (Quantitative)\n",
      "  11. SDQ_SDQ_Externalizing (Quantitative)\n",
      "  12. SDQ_SDQ_Generating_Impact (Quantitative)\n",
      "  13. SDQ_SDQ_Hyperactivity (Quantitative)\n",
      "  14. SDQ_SDQ_Internalizing (Quantitative)\n",
      "  15. SDQ_SDQ_Peer_Problems (Quantitative)\n",
      "      ... and 10 more\n",
      "\n",
      "--- Cross-Validation Evaluation ---\n",
      "Cross-validation F1-Macro scores: [0.73975045 0.74382113 0.75157053 0.7469459  0.77761912]\n",
      "Mean F1-Macro: 0.7519\n",
      "Median F1-Macro: 0.7469\n",
      "Std Dev F1-Macro: 0.0134\n",
      "\n",
      "Validation F1-Macro: 0.7791\n",
      "Test F1-Macro: 0.7314\n",
      "Test AUC: 0.7978\n",
      "Test Accuracy: 0.7531\n",
      "ADHD Precision: 0.8630\n",
      "ADHD Recall: 0.7590\n",
      "TESTING SFS WITH 21 FEATURES\n",
      "Running forward selection on 22 features...\n",
      "Target: Select 21 best features\n",
      "\n",
      "SFS completed in 12.4 seconds\n",
      "Selected 21 features:\n",
      "  - Quantitative: 17\n",
      "  - Categorical: 4\n",
      "\n",
      "Top selected features:\n",
      "   1. EHQ_EHQ_Total (Quantitative)\n",
      "   2. ColorVision_CV_Score (Quantitative)\n",
      "   3. APQ_P_APQ_P_CP (Quantitative)\n",
      "   4. APQ_P_APQ_P_ID (Quantitative)\n",
      "   5. APQ_P_APQ_P_INV (Quantitative)\n",
      "   6. APQ_P_APQ_P_OPD (Quantitative)\n",
      "   7. APQ_P_APQ_P_PM (Quantitative)\n",
      "   8. APQ_P_APQ_P_PP (Quantitative)\n",
      "   9. SDQ_SDQ_Conduct_Problems (Quantitative)\n",
      "  10. SDQ_SDQ_Difficulties_Total (Quantitative)\n",
      "  11. SDQ_SDQ_Emotional_Problems (Quantitative)\n",
      "  12. SDQ_SDQ_Externalizing (Quantitative)\n",
      "  13. SDQ_SDQ_Generating_Impact (Quantitative)\n",
      "  14. SDQ_SDQ_Hyperactivity (Quantitative)\n",
      "  15. SDQ_SDQ_Internalizing (Quantitative)\n",
      "      ... and 11 more\n",
      "\n",
      "--- Cross-Validation Evaluation ---\n",
      "Cross-validation F1-Macro scores: [0.7311784  0.74382113 0.74514061 0.74052632 0.75340136]\n",
      "Mean F1-Macro: 0.7428\n",
      "Median F1-Macro: 0.7438\n",
      "Std Dev F1-Macro: 0.0072\n",
      "\n",
      "Validation F1-Macro: 0.7554\n",
      "Test F1-Macro: 0.7276\n",
      "Test AUC: 0.7953\n",
      "Test Accuracy: 0.7490\n",
      "ADHD Precision: 0.8621\n",
      "ADHD Recall: 0.7530\n",
      "\n",
      "=== SFS Experiment Summary ===\n",
      "             mean_f1    val_f1   test_f1  test_auc  n_features\n",
      "n_features                                                    \n",
      "5           0.760718  0.756175  0.713885  0.803669           5\n",
      "10          0.764484  0.773842  0.697614  0.799014          10\n",
      "15          0.761888  0.752974  0.711206  0.798780          15\n",
      "20          0.751941  0.779124  0.731432  0.797762          20\n",
      "21          0.742814  0.755380  0.727644  0.795259          21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Test different numbers of features\n",
    "n_features_list = [5,10, 15, 20, 21]\n",
    "sfs_results = {}\n",
    "\n",
    "# Initialize base model for SFS\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "for n_features in n_features_list:\n",
    "   \n",
    "    print(f\"TESTING SFS WITH {n_features} FEATURES\")\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Sequential Feature Selector\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=base_model,\n",
    "        n_features_to_select=min(n_features, len(all_feature_cols)),\n",
    "        direction='forward',\n",
    "        scoring='f1_macro',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    print(f\"Running forward selection on {len(all_feature_cols)} features...\")\n",
    "    print(f\"Target: Select {min(n_features, len(all_feature_cols))} best features\")\n",
    "    \n",
    "    # Fit SFS on training data (scaled quantitative + categorical)\n",
    "    X_train_sfs = X_train_processed[all_feature_cols]\n",
    "    sfs.fit(X_train_sfs, y_train)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_mask = sfs.get_support()\n",
    "    selected_features = [col for col, selected in zip(all_feature_cols, selected_mask) if selected]\n",
    "    \n",
    "    sfs_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nSFS completed in {sfs_time:.1f} seconds\")\n",
    "    print(f\"Selected {len(selected_features)} features:\")\n",
    "    \n",
    "    # Categorize selected features\n",
    "    selected_quant = [f for f in selected_features if f in quant_cols]\n",
    "    selected_cat = [f for f in selected_features if f in cat_cols]\n",
    "    \n",
    "    print(f\"  - Quantitative: {len(selected_quant)}\")\n",
    "    print(f\"  - Categorical: {len(selected_cat)}\")\n",
    "    \n",
    "    print(f\"\\nTop selected features:\")\n",
    "    for i, feat in enumerate(selected_features[:15]):\n",
    "        feat_type = \"Quantitative\" if feat in quant_cols else \"Categorical\"\n",
    "        print(f\"  {i+1:2d}. {feat} ({feat_type})\")\n",
    "    if len(selected_features) > 10:\n",
    "        print(f\"      ... and {len(selected_features) - 10} more\")\n",
    "    \n",
    "   \n",
    "    print(f\"\\n--- Cross-Validation Evaluation ---\")\n",
    "    \n",
    "    # Prepare selected feature data\n",
    "    X_train_selected = X_train_processed[selected_features]\n",
    "    X_val_selected = X_val_processed[selected_features]\n",
    "    X_test_selected = X_test_processed[selected_features]\n",
    "    \n",
    "    # Cross-validation F1 scores\n",
    "    cv_f1_scores = cross_val_score(\n",
    "        base_model, \n",
    "        X_train_selected, \n",
    "        y_train, \n",
    "        cv=5, \n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_f1 = np.mean(cv_f1_scores)\n",
    "    median_f1 = np.median(cv_f1_scores)\n",
    "    std_f1 = np.std(cv_f1_scores)\n",
    "    \n",
    "    print(f\"Cross-validation F1-Macro scores: {cv_f1_scores}\")\n",
    "    print(f\"Mean F1-Macro: {mean_f1:.4f}\")\n",
    "    print(f\"Median F1-Macro: {median_f1:.4f}\")\n",
    "    print(f\"Std Dev F1-Macro: {std_f1:.4f}\")\n",
    "    \n",
    "    # Train final model for validation/test evaluation\n",
    "    final_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Validation set evaluation\n",
    "    y_val_pred = final_model.predict(X_val_selected)\n",
    "    val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_test_pred = final_model.predict(X_test_selected)\n",
    "    y_test_proba = final_model.predict_proba(X_test_selected)[:, 1]\n",
    "    test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    test_auc = auc(*roc_curve(y_test, y_test_proba)[:2])\n",
    "    \n",
    "    print(f\"\\nValidation F1-Macro: {val_report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test F1-Macro: {test_report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_report['accuracy']:.4f}\")\n",
    "    print(f\"ADHD Precision: {test_report['1']['precision']:.4f}\")\n",
    "    print(f\"ADHD Recall: {test_report['1']['recall']:.4f}\")\n",
    "    \n",
    " \n",
    "    sfs_results[n_features] = {\n",
    "        'selected_features': selected_features,\n",
    "        'selected_quant': selected_quant,\n",
    "        'selected_cat': selected_cat,\n",
    "        'cv_f1_scores': cv_f1_scores,\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'val_f1': val_report['macro avg']['f1-score'],\n",
    "        'test_f1': test_report['macro avg']['f1-score'],\n",
    "        'test_auc': test_auc,\n",
    "        'adhd_precision': test_report['1']['precision'],\n",
    "        'adhd_recall': test_report['1']['recall'],\n",
    "        'sfs_time': sfs_time,\n",
    "        'n_features': len(selected_features)\n",
    "    }\n",
    "\n",
    "comparison_df = pd.DataFrame.from_dict(sfs_results, orient='index')\n",
    "comparison_df.index.name = 'n_features'\n",
    "comparison_df = comparison_df.sort_index()\n",
    "print(\"\\n=== SFS Experiment Summary ===\")\n",
    "print(comparison_df[['mean_f1', 'val_f1', 'test_f1', 'test_auc', 'n_features']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02144e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BEST CONFIGURATION: 20 features\n",
      "Cross-validation F1-Macro: 0.7519 Â± 0.0134\n",
      "Test F1-Macro: 0.7314\n",
      "Test AUC: 0.7978\n",
      "ADHD Precision: 0.8630\n",
      "ADHD Recall: 0.7590\n",
      "\n",
      "Best selected features (20):\n",
      "\n",
      "Quantitative features (16):\n",
      "   1. ColorVision_CV_Score\n",
      "   2. APQ_P_APQ_P_CP\n",
      "   3. APQ_P_APQ_P_ID\n",
      "   4. APQ_P_APQ_P_INV\n",
      "   5. APQ_P_APQ_P_OPD\n",
      "   6. APQ_P_APQ_P_PM\n",
      "   7. APQ_P_APQ_P_PP\n",
      "   8. SDQ_SDQ_Conduct_Problems\n",
      "   9. SDQ_SDQ_Difficulties_Total\n",
      "  10. SDQ_SDQ_Emotional_Problems\n",
      "  11. SDQ_SDQ_Externalizing\n",
      "  12. SDQ_SDQ_Generating_Impact\n",
      "  13. SDQ_SDQ_Hyperactivity\n",
      "  14. SDQ_SDQ_Internalizing\n",
      "  15. SDQ_SDQ_Peer_Problems\n",
      "  16. SDQ_SDQ_Prosocial\n",
      "\n",
      "Categorical features (4):\n",
      "   1. Basic_Demos_Enroll_Year\n",
      "   2. Basic_Demos_Study_Site\n",
      "   3. PreInt_Demos_Fam_Child_Ethnicity\n",
      "   4. Sex_F\n"
     ]
    }
   ],
   "source": [
    "best_n_features = comparison_df.loc[comparison_df['test_f1'].idxmax(), 'n_features']\n",
    "best_results = sfs_results[best_n_features]\n",
    "\n",
    "print(f\"\\nBEST CONFIGURATION: {best_n_features} features\")\n",
    "print(f\"Cross-validation F1-Macro: {best_results['mean_f1']:.4f} Â± {best_results['std_f1']:.4f}\")\n",
    "print(f\"Test F1-Macro: {best_results['test_f1']:.4f}\")\n",
    "print(f\"Test AUC: {best_results['test_auc']:.4f}\")\n",
    "print(f\"ADHD Precision: {best_results['adhd_precision']:.4f}\")\n",
    "print(f\"ADHD Recall: {best_results['adhd_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest selected features ({len(best_results['selected_features'])}):\")\n",
    "print(f\"\\nQuantitative features ({len(best_results['selected_quant'])}):\")\n",
    "for i, feat in enumerate(best_results['selected_quant']):\n",
    "    print(f\"  {i+1:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(best_results['selected_cat'])}):\")\n",
    "for i, feat in enumerate(best_results['selected_cat']):\n",
    "    print(f\"  {i+1:2d}. {feat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
