{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e67ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_curve, auc\n",
    "from sklearn.model_selection import cross_val_score \n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb86346",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\04ama\\Downloads\\raw adhd data\\raw_dataset.csv\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66c9240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.drop(columns=['ADHD_Outcome', 'participant_id'], errors='ignore')\n",
    "y = df['ADHD_Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60d07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_cols = [col for col in df.columns if col.startswith('APQ_') or col.startswith('SDQ_') or col.startswith('EHQ_') or col.startswith('ColorVision')]\n",
    "cat_cols = [col for col in df.columns if col.startswith('PreInt_') or col.startswith('Basic_') or col.startswith('Handedness') or col.startswith('Sex_F')]\n",
    "conn_cols = list(df.iloc[:, 1:19902].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d841f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8efe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed = X_train.copy()\n",
    "X_val_processed = X_val.copy()\n",
    "X_test_processed = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ce016",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scaling quantitative features\")\n",
    "X_train_processed[quant_cols] = scaler.fit_transform(X_train_processed[quant_cols])\n",
    "X_val_processed[quant_cols] = scaler.transform(X_val_processed[quant_cols])\n",
    "X_test_processed[quant_cols] = scaler.transform(X_test_processed[quant_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c40472",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_cols = quant_cols + cat_cols\n",
    "X_train_processed[all_feature_cols] = imputer.fit_transform(X_train_processed[all_feature_cols])\n",
    "X_val_processed[all_feature_cols] = imputer.transform(X_val_processed[all_feature_cols])\n",
    "X_test_processed[all_feature_cols] = imputer.transform(X_test_processed[all_feature_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0384a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SIMPLE ADASYN IMPLEMENTATION\n",
    "# =============================================================================\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare data for ADASYN\n",
    "\n",
    "\n",
    "# Calculate original imbalance ratio\n",
    "print(\"\\nOriginal training imbalance:\")\n",
    "original_counts = Counter(y_train)\n",
    "original_ratio = max(original_counts.values()) / min(original_counts.values())\n",
    "print(f\"   • Original imbalance ratio: {original_ratio:.3f}:1\")\n",
    "\n",
    "print(\"\\ Applying ADASYN to training data...\")\n",
    "adasyn = ADASYN(n_neighbors=15, random_state=42, sampling_strategy='auto')\n",
    "X_train_balanced, y_train_balanced = adasyn.fit_resample(X_train_processed[all_feature_cols], y_train)\n",
    "\n",
    "# Calculate new imbalance ratio\n",
    "new_counts = Counter(y_train_balanced)\n",
    "new_ratio = max(new_counts.values()) / min(new_counts.values())\n",
    "print(f\"   • New imbalance ratio: {new_ratio:.3f}:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f7244a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Test different numbers of features\n",
    "n_features_list = [5,10, 15, 20, 21]\n",
    "sfs_results = {}\n",
    "\n",
    "# Initialize base model for SFS\n",
    "base_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "\n",
    "for n_features in n_features_list:\n",
    "   \n",
    "    print(f\"TESTING SFS WITH {n_features} FEATURES\")\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize Sequential Feature Selector\n",
    "    sfs = SequentialFeatureSelector(\n",
    "        estimator=base_model,\n",
    "        n_features_to_select=min(n_features, len(all_feature_cols)),\n",
    "        direction='forward',\n",
    "        scoring='f1_macro',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    print(f\"Running forward selection on {len(all_feature_cols)} features...\")\n",
    "    print(f\"Target: Select {min(n_features, len(all_feature_cols))} best features\")\n",
    "    \n",
    "    # Fit SFS on training data (scaled quantitative + categorical)\n",
    "    X_train_sfs = X_train_processed[all_feature_cols]\n",
    "    sfs.fit(X_train_sfs, y_train)\n",
    "    \n",
    "    # Get selected features\n",
    "    selected_mask = sfs.get_support()\n",
    "    selected_features = [col for col, selected in zip(all_feature_cols, selected_mask) if selected]\n",
    "    \n",
    "    sfs_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nSFS completed in {sfs_time:.1f} seconds\")\n",
    "    print(f\"Selected {len(selected_features)} features:\")\n",
    "    \n",
    "    # Categorize selected features\n",
    "    selected_quant = [f for f in selected_features if f in quant_cols]\n",
    "    selected_cat = [f for f in selected_features if f in cat_cols]\n",
    "    \n",
    "    print(f\"  - Quantitative: {len(selected_quant)}\")\n",
    "    print(f\"  - Categorical: {len(selected_cat)}\")\n",
    "    \n",
    "    print(f\"\\nTop selected features:\")\n",
    "    for i, feat in enumerate(selected_features[:15]):\n",
    "        feat_type = \"Quantitative\" if feat in quant_cols else \"Categorical\"\n",
    "        print(f\"  {i+1:2d}. {feat} ({feat_type})\")\n",
    "    if len(selected_features) > 10:\n",
    "        print(f\"      ... and {len(selected_features) - 10} more\")\n",
    "    \n",
    "   \n",
    "    print(f\"\\n--- Cross-Validation Evaluation ---\")\n",
    "    \n",
    "    # Prepare selected feature data\n",
    "    X_train_selected = X_train_processed[selected_features]\n",
    "    X_val_selected = X_val_processed[selected_features]\n",
    "    X_test_selected = X_test_processed[selected_features]\n",
    "    \n",
    "    # Cross-validation F1 scores\n",
    "    cv_f1_scores = cross_val_score(\n",
    "        base_model, \n",
    "        X_train_selected, \n",
    "        y_train, \n",
    "        cv=5, \n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    mean_f1 = np.mean(cv_f1_scores)\n",
    "    median_f1 = np.median(cv_f1_scores)\n",
    "    std_f1 = np.std(cv_f1_scores)\n",
    "    \n",
    "    print(f\"Cross-validation F1-Macro scores: {cv_f1_scores}\")\n",
    "    print(f\"Mean F1-Macro: {mean_f1:.4f}\")\n",
    "    print(f\"Median F1-Macro: {median_f1:.4f}\")\n",
    "    print(f\"Std Dev F1-Macro: {std_f1:.4f}\")\n",
    "    \n",
    "    # Train final model for validation/test evaluation\n",
    "    final_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "    final_model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Validation set evaluation\n",
    "    y_val_pred = final_model.predict(X_val_selected)\n",
    "    val_report = classification_report(y_val, y_val_pred, output_dict=True)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_test_pred = final_model.predict(X_test_selected)\n",
    "    y_test_proba = final_model.predict_proba(X_test_selected)[:, 1]\n",
    "    test_report = classification_report(y_test, y_test_pred, output_dict=True)\n",
    "    test_auc = auc(*roc_curve(y_test, y_test_proba)[:2])\n",
    "    \n",
    "    print(f\"\\nValidation F1-Macro: {val_report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test F1-Macro: {test_report['macro avg']['f1-score']:.4f}\")\n",
    "    print(f\"Test AUC: {test_auc:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_report['accuracy']:.4f}\")\n",
    "    print(f\"ADHD Precision: {test_report['1']['precision']:.4f}\")\n",
    "    print(f\"ADHD Recall: {test_report['1']['recall']:.4f}\")\n",
    "    \n",
    " \n",
    "    sfs_results[n_features] = {\n",
    "        'selected_features': selected_features,\n",
    "        'selected_quant': selected_quant,\n",
    "        'selected_cat': selected_cat,\n",
    "        'cv_f1_scores': cv_f1_scores,\n",
    "        'mean_f1': mean_f1,\n",
    "        'std_f1': std_f1,\n",
    "        'val_f1': val_report['macro avg']['f1-score'],\n",
    "        'test_f1': test_report['macro avg']['f1-score'],\n",
    "        'test_auc': test_auc,\n",
    "        'adhd_precision': test_report['1']['precision'],\n",
    "        'adhd_recall': test_report['1']['recall'],\n",
    "        'sfs_time': sfs_time,\n",
    "        'n_features': len(selected_features)\n",
    "    }\n",
    "\n",
    "comparison_df = pd.DataFrame.from_dict(sfs_results, orient='index')\n",
    "comparison_df.index.name = 'n_features'\n",
    "comparison_df = comparison_df.sort_index()\n",
    "print(\"\\n=== SFS Experiment Summary ===\")\n",
    "print(comparison_df[['mean_f1', 'val_f1', 'test_f1', 'test_auc', 'n_features']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02144e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n_features = comparison_df.loc[comparison_df['test_f1'].idxmax(), 'n_features']\n",
    "best_results = sfs_results[best_n_features]\n",
    "\n",
    "print(f\"\\nBEST CONFIGURATION: {best_n_features} features\")\n",
    "print(f\"Cross-validation F1-Macro: {best_results['mean_f1']:.4f} ± {best_results['std_f1']:.4f}\")\n",
    "print(f\"Test F1-Macro: {best_results['test_f1']:.4f}\")\n",
    "print(f\"Test AUC: {best_results['test_auc']:.4f}\")\n",
    "print(f\"ADHD Precision: {best_results['adhd_precision']:.4f}\")\n",
    "print(f\"ADHD Recall: {best_results['adhd_recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest selected features ({len(best_results['selected_features'])}):\")\n",
    "print(f\"\\nQuantitative features ({len(best_results['selected_quant'])}):\")\n",
    "for i, feat in enumerate(best_results['selected_quant']):\n",
    "    print(f\"  {i+1:2d}. {feat}\")\n",
    "\n",
    "print(f\"\\nCategorical features ({len(best_results['selected_cat'])}):\")\n",
    "for i, feat in enumerate(best_results['selected_cat']):\n",
    "    print(f\"  {i+1:2d}. {feat}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
