{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "%pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63650a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADHD Prediction Dataset - Feature Engineering Module\n",
    "\n",
    "This notebook performs feature engineering on the preprocessed ADHD dataset,\n",
    "including correlation analysis, feature selection, and model comparison to\n",
    "identify the optimal feature set and baseline model for ADHD prediction.\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: [Date]\n",
    "Project: ADHD Sex Prediction\n",
    "Input: Preprocessed dataset from Data Preparation module\n",
    "Output: Final engineered dataset and baseline model selection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEEDS = [21, 42, 100, 123, 2025]\n",
    "TEST_SIZE = 0.2\n",
    "CORRELATION_THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fb663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "def load_and_inspect_data():\n",
    "    # Load preprocessed data\n",
    "    data_path = r\"C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\notebooks\\Data Preparation\\balanced_train_data.csv\"\n",
    "    X_train = joblib.load('data/processed/X_train_balanced.pkl')\n",
    "    y_train = joblib.load('data/processed/y_train_balanced.pkl')\n",
    "    X_val = joblib.load('data/processed/X_val_processed.pkl')\n",
    "    y_val = joblib.load('data/processed/y_val.pkl')\n",
    "    X_test = joblib.load('data/processed/X_test_processed.pkl')\n",
    "    y_test = joblib.load('data/processed/y_test.pkl')\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        print(f\" Samples: {df.shape[0]:,}\")\n",
    "        print(f\" Features: {df.shape[1]:,}\")\n",
    "        \n",
    "       \n",
    "        return  df, X_train, y_train, X_val, y_val, X_test, y_test\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   âŒ Error: Dataset not found at {data_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error loading dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load the dataset\n",
    "df, X_train, y_train, X_val, y_val, X_test, y_test = load_and_inspect_data() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_correlations(df, threshold=CORRELATION_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Analyze correlations among quantitative features and visualize results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset with features to analyze\n",
    "    threshold : float\n",
    "        Correlation threshold for identifying highly correlated features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (correlation_matrix, highly_correlated_pairs)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Identify quantitative columns for correlation analysis\n",
    "    quant_cols = joblib.load('C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\notebooks\\Modelling & Evaluation\\quant_cols.json')\n",
    "    print(f\"Analyzing correlations among {len(quant_cols)} quantitative features...\")\n",
    "    \n",
    "    if not quant_cols:\n",
    "        print(f\" No quantitative features found for correlation analysis\")\n",
    "        return None, []\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr = df[quant_cols].corr()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    corr_pairs = corr.unstack()\n",
    "    high_corr = corr_pairs[(corr_pairs > threshold) & (corr_pairs < 1.0)]\n",
    "    \n",
    "    print(f\"\\n Correlation Analysis Results:\")\n",
    "    print(f\" Features analyzed: {len(quant_cols)}\")\n",
    "    print(f\" Correlation threshold: {threshold}\")\n",
    "    print(f\" Highly correlated pairs found: {len(high_corr)}\")\n",
    "    \n",
    "    if len(high_corr) > 0:\n",
    "        print(f\"\\n Highly Correlated Feature Pairs:\")\n",
    "        for (feat1, feat2), corr_val in high_corr.head(10).items():\n",
    "            print(f\" {feat1} -> {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(f\"No highly correlated features above threshold\")\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "   \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True,\n",
    "               cbar_kws={\"shrink\": .8})\n",
    "    plt.title(f'Quantitative Features Correlation Matrix\\n({len(quant_cols)} features)', \n",
    "              fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr, high_corr\n",
    "\n",
    "# Analyze correlations\n",
    "corr_matrix, high_corr_pairs = analyze_feature_correlations(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REDUNDANT FEATURE REMOVAL\n",
    "# =============================================================================\n",
    "\n",
    "def remove_highly_correlated_features(df, corr_matrix, threshold=CORRELATION_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features to reduce multicollinearity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset with features\n",
    "    corr_matrix : pd.DataFrame\n",
    "        Correlation matrix of features\n",
    "    threshold : float\n",
    "        Correlation threshold for feature removal\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (df_filtered, dropped_features) - Dataset after removal and list of dropped features\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    corr_pairs = corr_matrix.unstack()\n",
    "    high_corr = corr_pairs[(corr_pairs > threshold) & (corr_pairs < 1.0)]\n",
    "    \n",
    "    # Determine which features to drop\n",
    "    to_drop = set()\n",
    "    feature_pairs = []\n",
    "    \n",
    "    for (col1, col2), corr_val in high_corr.items():\n",
    "        if col1 not in to_drop and col2 not in to_drop:\n",
    "            feature_pairs.append((col1, col2, corr_val))\n",
    "            \n",
    "            # Drop feature with more missing values (or first one if equal)\n",
    "            missing_col1 = df[col1].isnull().sum()\n",
    "            missing_col2 = df[col2].isnull().sum()\n",
    "            \n",
    "            if missing_col1 > missing_col2:\n",
    "                to_drop.add(col1)\n",
    "                dropped_feature = col1\n",
    "                kept_feature = col2\n",
    "            elif missing_col2 > missing_col1:\n",
    "                to_drop.add(col2)\n",
    "                dropped_feature = col2\n",
    "                kept_feature = col1\n",
    "            else:\n",
    "                # If equal missing values, drop the first one alphabetically\n",
    "                if col1 < col2:\n",
    "                    to_drop.add(col2)\n",
    "                    dropped_feature = col2\n",
    "                    kept_feature = col1\n",
    "                else:\n",
    "                    to_drop.add(col1)\n",
    "                    dropped_feature = col1\n",
    "                    kept_feature = col2\n",
    "            \n",
    "            print(f\" Dropping {dropped_feature} (corr with {kept_feature}: {corr_val:.3f})\")\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    if to_drop:\n",
    "        print(f\"\\n Feature Removal Summary:\")\n",
    "        print(f\"  Features to drop: {len(to_drop)}\")\n",
    "        print(f\"  Dropped features: {sorted(list(to_drop))}\")\n",
    "        \n",
    "        df = df.drop(columns=list(to_drop))\n",
    "        \n",
    "        print(f\"\\n   ðŸ“ˆ Dataset Shape Changes:\")\n",
    "        print(f\"      â€¢ Before: {original_shape[0]:,} rows Ã— {original_shape[1]:,} columns\")\n",
    "        print(f\"      â€¢ After: {df.shape[0]:,} rows Ã— {df.shape[1]:,} columns\")\n",
    "        print(f\"      â€¢ Features removed: {len(to_drop)}\")\n",
    "        print(f\"      â€¢ Features retained: {df.shape[1] - original_shape[1] + len(to_drop)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"  No highly correlated features found to drop\")\n",
    "        df = df.copy()\n",
    "    \n",
    "    return df, list(to_drop)\n",
    "\n",
    "# Remove highly correlated features\n",
    "df, dropped_features = remove_highly_correlated_features(df, corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914336c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL COMPARISON AND EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_models_with_multiple_seeds(X_train, X_val, y_train, y_val, seeds=RANDOM_SEEDS):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models using preprocessed train/validation splits.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ† MODEL EVALUATION WITH MULTIPLE SEEDS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"SVM\": SVC(random_state=42),\n",
    "        \"XGBoost\": xgb.XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "        \"LightGBM\": lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n   ðŸ”„ Evaluating {name}:\")\n",
    "        \n",
    "        # Train on training set, evaluate on validation set\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "        \n",
    "        # Calculate macro F1-score\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        macro_f1 = report['macro avg']['f1-score']\n",
    "        \n",
    "        results[name] = {\n",
    "            'scores': [macro_f1],  # Single score since we're using fixed splits\n",
    "            'mean': macro_f1,\n",
    "            'std': 0.0,  # No std since single evaluation\n",
    "            'min': macro_f1,\n",
    "            'max': macro_f1\n",
    "        }\n",
    "        \n",
    "        print(f\"      ðŸ“Š Macro F1-Score: {macro_f1:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_model_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize model evaluation results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Model evaluation results from evaluate_models_with_multiple_seeds\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\" Model Performance\")\n",
    "    \n",
    "    # Sort models by mean performance\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "    \n",
    "    for rank, (name, stats) in enumerate(sorted_results, 1):\n",
    "        print(f\"      {rank}. {name}:\")\n",
    "        print(f\"    Mean Macro F1: {stats['mean']:.4f}\")\n",
    "        print(f\"     Std Deviation: {stats['std']:.4f}\")\n",
    "        print(f\"   Stability: {'High' if stats['std'] < 0.01 else 'Medium' if stats['std'] < 0.02 else 'Low'}\")\n",
    "    \n",
    "    # Identify best model\n",
    "    best_model = sorted_results[0][0]\n",
    "    best_stats = sorted_results[0][1]\n",
    "    \n",
    "    print(f\" Best Model Selection:\")\n",
    "    print(f\"{best_model}\")\n",
    "    print(f\"      Mean Performance: {best_stats['mean']:.4f}\")\n",
    "    print(f\"      Stability (Std): {best_stats['std']:.4f}\")\n",
    "    \n",
    "    # Create visualization\n",
    " \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot of mean scores with error bars\n",
    "    model_names = [name for name, _ in sorted_results]\n",
    "    mean_scores = [stats['mean'] for _, stats in sorted_results]\n",
    "    std_scores = [stats['std'] for _, stats in sorted_results]\n",
    "    \n",
    "    bars = ax1.bar(model_names, mean_scores, yerr=std_scores, capsize=5, \n",
    "                   color=['gold' if i == 0 else 'lightblue' for i in range(len(model_names))])\n",
    "    ax1.set_title('Model Performance Comparison\\n(Mean Macro F1-Score with Standard Deviation)')\n",
    "    ax1.set_ylabel('Macro F1-Score')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean_val, std_val in zip(bars, mean_scores, std_scores):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + std_val + 0.001,\n",
    "                f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Box plot for score distributions\n",
    "    score_data = [stats['scores'] for _, stats in sorted_results]\n",
    "    box_plot = ax2.boxplot(score_data, labels=model_names, patch_artist=True)\n",
    "\n",
    "    # Coloring  the best model's box gold\n",
    "    for i, patch in enumerate(box_plot['boxes']):\n",
    "        if i == 0:  # Best model\n",
    "            patch.set_facecolor('gold')\n",
    "            patch.set_alpha(0.7)\n",
    "        else:\n",
    "            patch.set_facecolor('lightblue')\n",
    "            patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_title('Score Distribution Across Random Seeds')\n",
    "    ax2.set_ylabel('Macro F1-Score')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, best_stats\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_results = evaluate_models_with_multiple_seeds(X_train, X_val, y_train, y_val)\n",
    "\n",
    "# Analyze results\n",
    "best_model_name, best_model_stats = analyze_model_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3730afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL DATASET EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "def export_final_dataset(df, filename='final_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Export the final engineered dataset for model training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Final processed dataset\n",
    "    filename : str\n",
    "        Output filename\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Save final dataset\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error exporting dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Export final dataset\n",
    "export_final_dataset(df, 'final_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22087b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
