{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa6482",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install xgboost\n",
    "%pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63650a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADHD Prediction Dataset - Feature Engineering Module\n",
    "\n",
    "This notebook performs feature engineering on the preprocessed ADHD dataset,\n",
    "including correlation analysis, feature selection, and model comparison to\n",
    "identify the optimal feature set and baseline model for ADHD prediction.\n",
    "\n",
    "Author: [Your Name]\n",
    "Date: [Date]\n",
    "Project: ADHD Sex Prediction\n",
    "Input: Preprocessed dataset from Data Preparation module\n",
    "Output: Final engineered dataset and baseline model selection\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND SETTINGS\n",
    "# =============================================================================\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEEDS = [21, 42, 100, 123, 2025]\n",
    "TEST_SIZE = 0.2\n",
    "CORRELATION_THRESHOLD = 0.7\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05fb663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_inspect_data():\n",
    "    # Load preprocessed data\n",
    "    data_path = r\"C:\\Users\\04ama\\OneDrive\\chemistry\\ADHD_SEX_Prediction\\notebooks\\Data Preparation\\balanced_adhd_dataset.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        \n",
    "        print(f\" Samples: {df.shape[0]:,}\")\n",
    "        print(f\" Features: {df.shape[1]:,}\")\n",
    "        \n",
    "        return df \n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"   âŒ Error: Dataset not found at {data_path}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error loading dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load the dataset\n",
    "df = load_and_inspect_data()  # This returns None!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FEATURE CORRELATION ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "def analyze_feature_correlations(df, threshold=CORRELATION_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Analyze correlations among quantitative features and visualize results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset with features to analyze\n",
    "    threshold : float\n",
    "        Correlation threshold for identifying highly correlated features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (correlation_matrix, highly_correlated_pairs)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Identify quantitative columns for correlation analysis\n",
    "    quant_cols = [col for col in df.columns if \n",
    "                 (col.startswith('APQ_') or col.startswith('SDQ_') or \n",
    "                  col.startswith('EHQ_') or col.startswith('ColorVision')) \n",
    "                 and col in df.columns]\n",
    "    \n",
    "    print(f\"Analyzing correlations among {len(quant_cols)} quantitative features...\")\n",
    "    \n",
    "    if not quant_cols:\n",
    "        print(f\" No quantitative features found for correlation analysis\")\n",
    "        return None, []\n",
    "    \n",
    "    # Calculate correlation matrix\n",
    "    corr = df[quant_cols].corr()\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    corr_pairs = corr.unstack()\n",
    "    high_corr = corr_pairs[(corr_pairs > threshold) & (corr_pairs < 1.0)]\n",
    "    \n",
    "    print(f\"\\n Correlation Analysis Results:\")\n",
    "    print(f\" Features analyzed: {len(quant_cols)}\")\n",
    "    print(f\" Correlation threshold: {threshold}\")\n",
    "    print(f\" Highly correlated pairs found: {len(high_corr)}\")\n",
    "    \n",
    "    if len(high_corr) > 0:\n",
    "        print(f\"\\n Highly Correlated Feature Pairs:\")\n",
    "        for (feat1, feat2), corr_val in high_corr.head(10).items():\n",
    "            print(f\" {feat1} -> {feat2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(f\"No highly correlated features above threshold\")\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "   \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True,\n",
    "               cbar_kws={\"shrink\": .8})\n",
    "    plt.title(f'Quantitative Features Correlation Matrix\\n({len(quant_cols)} features)', \n",
    "              fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr, high_corr\n",
    "\n",
    "# Analyze correlations\n",
    "corr_matrix, high_corr_pairs = analyze_feature_correlations(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# REDUNDANT FEATURE REMOVAL\n",
    "# =============================================================================\n",
    "\n",
    "def remove_highly_correlated_features(df, corr_matrix, threshold=CORRELATION_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Remove highly correlated features to reduce multicollinearity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset with features\n",
    "    corr_matrix : pd.DataFrame\n",
    "        Correlation matrix of features\n",
    "    threshold : float\n",
    "        Correlation threshold for feature removal\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (df_filtered, dropped_features) - Dataset after removal and list of dropped features\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    corr_pairs = corr_matrix.unstack()\n",
    "    high_corr = corr_pairs[(corr_pairs > threshold) & (corr_pairs < 1.0)]\n",
    "    \n",
    "    # Determine which features to drop\n",
    "    to_drop = set()\n",
    "    feature_pairs = []\n",
    "    \n",
    "    for (col1, col2), corr_val in high_corr.items():\n",
    "        if col1 not in to_drop and col2 not in to_drop:\n",
    "            feature_pairs.append((col1, col2, corr_val))\n",
    "            \n",
    "            # Drop feature with more missing values (or first one if equal)\n",
    "            missing_col1 = df[col1].isnull().sum()\n",
    "            missing_col2 = df[col2].isnull().sum()\n",
    "            \n",
    "            if missing_col1 > missing_col2:\n",
    "                to_drop.add(col1)\n",
    "                dropped_feature = col1\n",
    "                kept_feature = col2\n",
    "            elif missing_col2 > missing_col1:\n",
    "                to_drop.add(col2)\n",
    "                dropped_feature = col2\n",
    "                kept_feature = col1\n",
    "            else:\n",
    "                # If equal missing values, drop the first one alphabetically\n",
    "                if col1 < col2:\n",
    "                    to_drop.add(col2)\n",
    "                    dropped_feature = col2\n",
    "                    kept_feature = col1\n",
    "                else:\n",
    "                    to_drop.add(col1)\n",
    "                    dropped_feature = col1\n",
    "                    kept_feature = col2\n",
    "            \n",
    "            print(f\" Dropping {dropped_feature} (corr with {kept_feature}: {corr_val:.3f})\")\n",
    "    \n",
    "    # Remove highly correlated features\n",
    "    original_shape = df.shape\n",
    "    \n",
    "    if to_drop:\n",
    "        print(f\"\\n Feature Removal Summary:\")\n",
    "        print(f\"  Features to drop: {len(to_drop)}\")\n",
    "        print(f\"  Dropped features: {sorted(list(to_drop))}\")\n",
    "        \n",
    "        df = df.drop(columns=list(to_drop))\n",
    "        \n",
    "        print(f\"\\n   ðŸ“ˆ Dataset Shape Changes:\")\n",
    "        print(f\"      â€¢ Before: {original_shape[0]:,} rows Ã— {original_shape[1]:,} columns\")\n",
    "        print(f\"      â€¢ After: {df.shape[0]:,} rows Ã— {df.shape[1]:,} columns\")\n",
    "        print(f\"      â€¢ Features removed: {len(to_drop)}\")\n",
    "        print(f\"      â€¢ Features retained: {df.shape[1] - original_shape[1] + len(to_drop)}\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"  No highly correlated features found to drop\")\n",
    "        df = df.copy()\n",
    "    \n",
    "    return df, list(to_drop)\n",
    "\n",
    "# Remove highly correlated features\n",
    "df, dropped_features = remove_highly_correlated_features(df, corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914336c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL COMPARISON AND EVALUATION\n",
    "# =============================================================================\n",
    "\n",
    "def prepare_features_target(df, target_col='ADHD_Outcome'):\n",
    "    \"\"\"\n",
    "    Prepare feature matrix and target vector for model training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Dataset with features and target\n",
    "    target_col : str\n",
    "        Name of target column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (X, y) - Feature matrix and target vector\n",
    "    \"\"\"\n",
    "  \n",
    "  \n",
    "    \n",
    "    # Prepare features (exclude target and identifier columns)\n",
    "    exclude_cols = [target_col, 'participant_id']\n",
    "    feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"\\n Target Variable Summary:\")\n",
    "    target_counts = y.value_counts().sort_index()\n",
    "    target_props = y.value_counts(normalize=True).sort_index()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def evaluate_models_with_multiple_seeds(X, y, seeds=RANDOM_SEEDS, test_size=TEST_SIZE):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models across different random seeds for stability assessment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : pd.DataFrame\n",
    "        Feature matrix\n",
    "    y : pd.Series\n",
    "        Target vector\n",
    "    seeds : list\n",
    "        List of random seeds for evaluation\n",
    "    test_size : float\n",
    "        Test set proportion\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Model evaluation results\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Define models to evaluate\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"SVM\": SVC(),\n",
    "        \"XGBoost\": xgb.XGBClassifier(random_state=42, enable_categorical=True),\n",
    "        \"LightGBM\": lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "    }\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\n Evaluating {name}:\")\n",
    "        macro_f1_scores = []\n",
    "        \n",
    "        for i, seed in enumerate(seeds, 1):\n",
    "            # Split data with current seed\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y, test_size=test_size, random_state=seed, stratify=y\n",
    "            )\n",
    "            \n",
    "            # Train and evaluate model\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate macro F1-score\n",
    "            report = classification_report(y_test, y_pred, output_dict=True)\n",
    "            macro_f1 = report['macro avg']['f1-score']\n",
    "            macro_f1_scores.append(macro_f1)\n",
    "            \n",
    "            print(f\" Seed {seed:4d} (Run {i}): F1 = {macro_f1:.4f}\")\n",
    "        \n",
    "        # Calculate statistics across seeds\n",
    "        mean_f1 = np.mean(macro_f1_scores)\n",
    "        std_f1 = np.std(macro_f1_scores)\n",
    "        min_f1 = np.min(macro_f1_scores)\n",
    "        max_f1 = np.max(macro_f1_scores)\n",
    "        \n",
    "        results[name] = {\n",
    "            'scores': macro_f1_scores,\n",
    "            'mean': mean_f1,\n",
    "            'std': std_f1,\n",
    "            'min': min_f1,\n",
    "            'max': max_f1\n",
    "        }\n",
    "        \n",
    "        print(f\"  Summary: Mean = {mean_f1:.4f} Â± {std_f1:.4f}\")\n",
    "        print(f\"Range = [{min_f1:.4f}, {max_f1:.4f}]\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_model_results(results):\n",
    "    \"\"\"\n",
    "    Analyze and visualize model evaluation results.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results : dict\n",
    "        Model evaluation results from evaluate_models_with_multiple_seeds\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\" Model Performance\")\n",
    "    \n",
    "    # Sort models by mean performance\n",
    "    sorted_results = sorted(results.items(), key=lambda x: x[1]['mean'], reverse=True)\n",
    "    \n",
    "    for rank, (name, stats) in enumerate(sorted_results, 1):\n",
    "        print(f\"      {rank}. {name}:\")\n",
    "        print(f\"    Mean Macro F1: {stats['mean']:.4f}\")\n",
    "        print(f\"     Std Deviation: {stats['std']:.4f}\")\n",
    "        print(f\"   Stability: {'High' if stats['std'] < 0.01 else 'Medium' if stats['std'] < 0.02 else 'Low'}\")\n",
    "    \n",
    "    # Identify best model\n",
    "    best_model = sorted_results[0][0]\n",
    "    best_stats = sorted_results[0][1]\n",
    "    \n",
    "    print(f\" Best Model Selection:\")\n",
    "    print(f\"{best_model}\")\n",
    "    print(f\"      Mean Performance: {best_stats['mean']:.4f}\")\n",
    "    print(f\"      Stability (Std): {best_stats['std']:.4f}\")\n",
    "    \n",
    "    # Create visualization\n",
    " \n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Bar plot of mean scores with error bars\n",
    "    model_names = [name for name, _ in sorted_results]\n",
    "    mean_scores = [stats['mean'] for _, stats in sorted_results]\n",
    "    std_scores = [stats['std'] for _, stats in sorted_results]\n",
    "    \n",
    "    bars = ax1.bar(model_names, mean_scores, yerr=std_scores, capsize=5, \n",
    "                   color=['gold' if i == 0 else 'lightblue' for i in range(len(model_names))])\n",
    "    ax1.set_title('Model Performance Comparison\\n(Mean Macro F1-Score with Standard Deviation)')\n",
    "    ax1.set_ylabel('Macro F1-Score')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, mean_val, std_val in zip(bars, mean_scores, std_scores):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + std_val + 0.001,\n",
    "                f'{mean_val:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Box plot for score distributions\n",
    "    score_data = [stats['scores'] for _, stats in sorted_results]\n",
    "    box_plot = ax2.boxplot(score_data, labels=model_names, patch_artist=True)\n",
    "    \n",
    "    # Color the best model's box differently\n",
    "    for i, patch in enumerate(box_plot['boxes']):\n",
    "        if i == 0:  # Best model\n",
    "            patch.set_facecolor('gold')\n",
    "            patch.set_alpha(0.7)\n",
    "        else:\n",
    "            patch.set_facecolor('lightblue')\n",
    "            patch.set_alpha(0.7)\n",
    "    \n",
    "    ax2.set_title('Score Distribution Across Random Seeds')\n",
    "    ax2.set_ylabel('Macro F1-Score')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, best_stats\n",
    "\n",
    "# Prepare features and target\n",
    "X, y = prepare_features_target(df)\n",
    "\n",
    "# Evaluate models\n",
    "evaluation_results = evaluate_models_with_multiple_seeds(X, y)\n",
    "\n",
    "# Analyze results\n",
    "best_model_name, best_model_stats = analyze_model_results(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632044b9",
   "metadata": {},
   "source": [
    "# testing models after feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3730afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FINAL DATASET EXPORT\n",
    "# =============================================================================\n",
    "\n",
    "def export_final_dataset(df, filename='final_dataset.csv'):\n",
    "    \"\"\"\n",
    "    Export the final engineered dataset for model training.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Final processed dataset\n",
    "    filename : str\n",
    "        Output filename\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Save final dataset\n",
    "        df.to_csv(filename, index=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  Error exporting dataset: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Export final dataset\n",
    "export_final_dataset(df, 'final_dataset.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf426883",
   "metadata": {},
   "source": [
    "## Model Stability and Selection Conclusion\n",
    "\n",
    "Based on repeated experiments with different random seeds, Logistic Regression achieved the highest mean macro F1-score and demonstrated good stability compared to other models. This indicates that Logistic Regression is the most effective and reliable baseline model for this dataset, outperforming more complex models such as Random Forest, SVM, XGBoost, and LightGBM. Therefore, Logistic Regression is recommended as the primary model for further analysis and optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a22087b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
